---
layout: default
title: "[중급프로젝트] RAG 기반 정부나라장터 입찰공고 분석 시스템 - 데이터 수집 및 전처리"
description: "[중급프로젝트] RAG 기반 정부나라장터 입찰공고 분석 시스템 - 데이터 수집 및 전처리"
date: 2025-11-25
cache-control: no-cache
expires: 0
pragma: no-cache
author: "신승목 (데이터 엔지니어)"
---

## 2. 데이터 수집 및 전처리 (신승목)

### 2.1. 문서 수집 전략

#### 2.1.1. PDF 및 HWP 파일 수집 프로세스

본 프로젝트에서는 데이터 수집 시 정부나라장터에서 제공하는 입찰 공고 문서를 체계적으로 수집하고 처리합니다. 정부나라장터는 공공데이터포털을 통해 입찰 공고 정보 조회 API를 제공하고 있으며, 이를 활용하여 자동화된 문서 수집이 가능합니다.

문서 수집을 위해 먼저 공공데이터포털의 입찰공고목록정보조회 API를 호출하여 최신 공고 목록을 조회합니다. 이 API는 응답 시 공고번호, 공고명, 발주기관, 입찰마감일, 첨부파일 정보 등의 메타데이터를 제공하는데, 응답에 포함된 첨부파일 URL을 통해 제안요청서 원본 파일을 다운로드할 수 있습니다.

본 프로젝트에서는 PDF와 HWP 파일을 불러와서 마크다운으로 DB에 저장하도록 구현하였습니다. PDF 문서는 PyMuPDF 라이브러리를 이용하였고 HWP 문서는 자체 라이브러리를 개발하여 활용하였습니다.

#### 2.1.2. 파일 해시 기반 중복 검사

추후에 대량의 문서를 처리할 것을 상정하여 중복된 문서를 여러 번 처리하는 일을 방지하기 위해 SHA-256 해시 알고리즘을 기반으로 중복 문서를 감지하고 제거하는 기능을 구현하였습니다.

SHA-256은 암호학적 해시 함수로, 입력 파일의 내용에 기반하여 64자리 16진수 문자열을 생성합니다. 이 해시값은 파일의 고유한 지문과 같은 역할을 하며, 파일 내용이 단 한 바이트라도 다르면 완전히 다른 해시값이 생성됩니다. 반대로 파일명이나 메타데이터가 달라도 내용이 동일하다면 같은 해시값을 갖게 됩니다.

중복 검사 프로세스는 다음과 같이 진행됩니다. 새로운 문서 파일이 입력되면 DocumentProcessor가 해시값을 계산하고, 이 해시값이 DocumentsDB에 이미 존재하는 경우 해당 파일은 중복으로 판단되어 Markdown 변환 및 DB 저장 작업을 수행하지 않습니다.

이러한 방식은 세 가지 장점을 제공합니다.
1. 파일명이 다르더라도 내용이 동일한 문서를 정확히 식별할 수 있습니다. 동일한 제안요청서가 다른 이름으로 재공고되거나, 다른 경로에서 다운로드된 경우에도 중복을 감지할 수 있습니다.
2. 해시 기반 조회는 데이터베이스 인덱스를 활용하여 매우 빠르게 수행됩니다. 수만 건의 문서가 저장되어 있어도 밀리초 단위로 중복 여부를 판단할 수 있습니다.
3. 해시값은 파일 크기와 무관하게 항상 64자리로 고정되어 있어 저장 공간을 효율적으로 사용합니다.

#### 2.1.3. 메타데이터 추출 및 관리

문서 처리 과정에서 원본 파일의 메타데이터를 체계적으로 추출하고 관리하는 것은 향후 검색 및 필터링 기능의 핵심 기반이 됩니다. 본 시스템에서는 파일 수준의 메타데이터와 문서 수준의 메타데이터를 구분하여 관리합니다.

파일 수준 메타데이터는 파일 자체의 물리적 속성을 나타냅니다.
- 파일 해시값, 파일명, 파일 크기, 총 페이지 수, 생성 시각, 수정 시각 등이 포함됩니다.
- 이러한 정보는 DocumentsDB의 TB_DOCUMENTS 테이블에 저장되며, file_hash를 기본 키로 사용하여 각 파일을 고유하게 식별합니다.

문서 메타데이터에 접근하기 위해 PyMuPDF 라이브러리를 사용하여 PDF 파일을 엽니다.
- PDF 표준에 따른 문서 메타데이터에는 제목, 저자, 주제, 키워드, 생성 도구, 생성 날짜, 수정 날짜 등의 정보가 포함될 수 있습니다. 
- 그러나 정부 나라장터의 공고 문서들은 이러한 메타데이터가 일관되게 작성되어 있지 않고 비어 있거나 의미 없는 값으로 채워져 있는 경우가 빈번합니다.

이러한 한계를 보완하기 위해 파일명에서 추출한 정보를 활용합니다. 파일명으로부터 발주기관, 사업명, 사업 유형 등의 정보를 추론할 수 있습니다. 예를 들어 파일명에 "정보시스템", "구축", "용역" 등의 키워드가 포함되어 있다면 이는 정보시스템 구축 용역 사업임을 나타냅니다.

총 페이지 수는 문서의 규모를 파악하는 중요한 지표입니다. PyMuPDF를 통해 PDF 파일을 열면 page_count 속성으로 전체 페이지 수를 확인할 수 있습니다. 이 정보는 사용자가 문서의 분량을 사전에 파악하고, 처리 시간을 예측하는 데 도움을 줍니다. 또한 페이지 수가 비정상적으로 많거나 적은 문서를 필터링하는 데도 활용됩니다. 한글 문서는 특성 상 페이지 정보가 없기 때문에 사용자가 임의로 페이지 정보를 추가해 주어야 합니다. 현재는 마크다운으로 변환되고 뒤에서 언급할 최소 전처리 후의 내용을 기준으로 40줄마다 페이지 정보를 추가하도록 구현하였습니다.

생성 시각과 수정 시각은 KST(Korean Standard Time) 기준으로 기록됩니다. Python의 datetime 모듈과 pytz 라이브러리를 사용하여 UTC 시각을 KST로 변환하며, 데이터베이스에는 ISO 8601 형식의 문자열로 저장됩니다. 이를 통해 문서가 언제 시스템에 추가되었고 언제 마지막으로 갱신되었는지를 명확히 추적할 수 있습니다.

메타데이터 관리는 단순한 정보 저장을 넘어 시스템의 투명성과 데이터 추적 성능을 높이는 역할을 합니다. 사용자는 메타데이터를 통해 검색 결과의 출처를 명확히 파악할 수 있으며, 시스템 관리자는 메타데이터 통계를 통해 데이터베이스의 상태를 모니터링할 수 있습니다.

### 2.2. 원본 전처리

#### 2.2.1. Markdown 변환 (pymupdf4llm, hwp_to_markdown)

문서를 기계가 처리할 수 있는 형태로 변환하는 것은 RAG 시스템 구축의 첫 번째 관문입니다. 복잡한 구조의 문서를 단순한 텍스트로 추출하면 문서의 구조적 정보가 손실되어 이후 처리 단계에서 문제가 발생할 수 있습니다.

본 시스템에서는 이후 전처리 단계에서 다루기 용이하도록 문서의 내용을 Markdown 텍스트로 변환하고자 합니다. Markdown은 간단한 문법으로 문서의 구조를 표현할 수 있는 경량 마크업 언어로, 헤더, 리스트, 테이블, 강조 등의 요소를 명확히 구분할 수 있습니다. 이를 통해 원본 문서의 계층 구조와 의미를 최대한 보존할 수 있습니다.<br>
또한 Markdown 텍스트는 가독성이 높고 구조화되어 있어서 간단한 정규 표현식이나 문자열 처리를 통해 헤더를 기준으로 섹션을 분리하거나, 테이블 데이터를 파싱하거나, 리스트 항목을 추출하는 등의 작업을 수행할 수 있습니다.

본 시스템에서는 PDF를 마크다운으로 변환할 때 pymupdf4llm 라이브러리를 사용합니다. pymupdf4llm은 PyMuPDF 라이브러리를 기반으로 하며, LLM 친화적인 Markdown 변환에 최적화되어 있습니다. 일반적인 텍스트 추출 라이브러리와 달리, pymupdf4llm은 다음과 같은 고급 기능을 제공합니다.
1. 문서의 논리적 구조를 분석하여 제목과 본문을 구분하고, 적절한 Markdown 헤더 레벨을 할당합니다.
2. 테이블 구조를 인식하여 Markdown 테이블 문법으로 변환합니다.
3. 리스트 항목을 감지하여 순서 있는 리스트와 순서 없는 리스트로 구분합니다.
4. 이미지는 대체 텍스트로 표시하거나 제거하는 옵션을 제공합니다.

페이지 단위로 마크다운 변환을 수행합니다. DocumentProcessor의 markdown_with_progress_pdf 메서드에서 PDF 파일을 열고 각 페이지를 순회하면서 pymupdf4llm.to_markdown 함수를 호출합니다. 이 함수는 페이지의 레이아웃을 분석하고 텍스트 블록을 추출한 후, 적절한 Markdown 요소로 변환합니다. 각 페이지의 변환이 완료되면 진행 상황이 콜백 함수를 통해 UI에 전달되어 사용자가 실시간으로 처리 상태를 확인할 수 있습니다.

기존 HWP 파일 처리 라이브러리가 텍스트만 처리할 수 있는는 한계를 극복하고자 HWP 문서의 텍스트와 표 내용을 parsing 하여 마크다운으로 변환할 수 있는 라이브러리를 직접 개발하여 pip로 설치할 수 있게 구현하였습니다. (개발자: 김명환)<br>
라이브러리 이름은 helper-hwp이고, hwp_to_markdown 메서드에 처리할 한글 파일의 경로를 입력하면 문서 내의 텍스트와 표를 parsing 하여 마크다운을 반환하도록 하였습니다.

```python
# 라이브러리 설치
pip install helper-hwp

# import
from helper_hwp import hwp_to_markdown

# 마크다운 변환
markdown = hwp_to_markdown('example.hwp')
```

마크다운 변환 과정에서 발생할 수 있는 예외 상황도 고려하여 반영하였습니다. 스캔된 이미지로만 구성된 페이지는 텍스트를 추출할 수 없고, 빈 페이지로 감지하여 ERROR_PAGE_MARKER 또는 EMPTY_PAGE_MARKER를 삽입합니다. 또한 암호화되거나 손상된 페이지를 만나면 오류를 로깅하고 해당 페이지를 처리하지 않고 건너뜁니다. 이를 통해 일부 페이지의 문제로 인해 전체 문서 처리가 중단되는 것을 방지합니다.

#### 2.2.2. DocumentProcessor 기반 최소 전처리

HWP와 PDF에서 Markdown으로 변환된 직후의 텍스트는 아직 많은 불필요한 요소를 포함하고 있습니다. 과도한 공백, 연속된 개행, 라인 앞뒤의 공백 등은 텍스트의 가독성을 해치고 토큰 수를 불필요하게 증가시킵니다. DocumentProcessor의 clean_markdown_text 메서드는 이러한 요소들을 제거하는 최소한의 전처리를 수행합니다.

최소 전처리는 원본 텍스트의 내용과 구조를 최대한 보존하면서 형식적인 문제만을 해결하는 것을 목표로 세 가지 주요 작업을 수행합니다.

첫 번째는 공백 및 탭의 정규화입니다. 연속된 공백이나 탭은 단일 공백으로 변환됩니다. 정규표현식 패턴 r'[ \t]+'를 사용하여 공백과 탭이 하나 이상 연속된 경우를 찾아 단일 공백으로 치환합니다. 예를 들어 "사업    기간"이라는 텍스트는 "사업 기간"으로 정규화됩니다. 이는 불필요한 공백으로 인한 토큰 낭비를 방지하면서도 단어 간 구분은 유지합니다.

두 번째는 연속된 개행 축소입니다. 문서 변환 과정에서 발생한 과도한 빈 줄을 제거하여 텍스트를 압축합니다. 정규표현식 패턴 r'\n{3,}'를 사용하여 세 번 이상 연속된 개행을 두 번의 개행으로 치환합니다. 이는 문단 구분은 유지하면서도 과도한 빈 공간을 제거합니다. 만약 기존 문서에 다섯 줄의 빈 줄이 있었다면, 두 줄로 줄어들어 해당 내용을 처리할 때 필요한 토큰 수를 줄일 수 있습니다.

세 번째는 각 라인의 앞뒤 공백 제거입니다. 텍스트를 줄 단위로 분리한 후 각 줄의 strip 메서드를 호출하여 앞뒤 공백을 제거하고, 다시 줄바꿈으로 결합합니다. 이를 통해 라인 시작이나 끝에 불필요하게 삽입된 공백을 제거할 수 있습니다.

해당 단계는 최소 전처리이기 때문에 텍스트의 의미나 구조를 변경할 수 있는 작업은 이 단계에서 수행하지 않고, 문서 내의 필요한 내용을 최대한 다음 단계에 전달하는 것에 집중합니다.

예를 들어 Markdown 요소의 제거, HTML 태그 제거, 특수문자 처리 등은 다음 단계인 EmbeddingProcessor의 전처리에서 수행됩니다. 이러한 분리는 원본 데이터를 최대한 보존하여 향후 다른 방식의 전처리가 필요할 때 원본에서 다시 시작할 수 있도록 합니다.

실제 정부나라장터 문서에 적용한 결과를 살펴보면 그 효과를 명확히 알 수 있습니다. 원본 PDF에서 변환된 직후의 텍스트는 "   ㅇ  (사 업 명)   2025 구미아시아육상경기선수권대회    종합정보시스템"과 같이 불규칙한 공백이 포함되어 있었습니다. 최소 전처리를 거친 후에는 "ㅇ (사 업 명) 2025 구미아시아육상경기선수권대회 종합정보시스템"으로 정리되어 가독성이 크게 향상되었습니다.

#### 2.2.3. 페이지 마커 삽입 전략

RAG 시스템에서 사용자의 질문과 직접 연관된 항목을 사용자에게 전달할 때 검색된 정보가 원본 문서의 몇 페이지에 있었는지 알려주는 것은 사용자 경험에 굉장히 도움이 됩니다. 사용자에게 "공고문.pdf의 15페이지를 참조하세요"라고 안내하는 것과 단순히 "공고문.pdf를 참조하세요"라고 하는 것은 사용자 경험에서 큰 차이가 있습니다. 하지만 Markdown으로 변환된 문서는 하나의 긴 텍스트 스트림으로 존재하기 때문에 검색 결과의 출처를 명시할 때 페이지 정보를 담지 못해서 큰 문제가 됩니다.

이러한 문제를 해결하기 위해 페이지 마커를 삽입하였습니다. 각 페이지의 변환된 텍스트 앞에는 고유한 페이지 마커가 삽입되어 페이지 경계를 명시합니다. 페이지 마커의 형식은 "--- 페이지 N ---"이며, N은 페이지 번호를 나타냅니다. 이 형식은 시각적으로 명확하고, 정규표현식으로 쉽게 감지할 수 있으며, 실제 문서 내용과 충돌할 가능성이 낮도록 설계되었습니다.

페이지 마커는 두 가지 중요한 역할을 수행합니다.
1. 청킹 단계에서의 페이지 단위 분할입니다. EmbeddingProcessor는 페이지 마커를 기준으로 텍스트를 분리하여 각 페이지를 독립적으로 처리할 수 있습니다. 이를 통해 페이지 경계를 넘나드는 청크 생성을 방지하고, 각 청크가 어느 페이지 범위에 속하는지 명확히 추적할 수 있습니다.
2. 검색 결과의 출처 표시입니다. 사용자가 질의를 입력하면 시스템은 관련 청크를 검색하고 LLM 응답을 생성합니다. 이때 각 청크의 메타데이터에는 start_page와 end_page 정보가 포함되어 있어, "이 정보는 공고문.pdf의 15페이지에서 17페이지에 있습니다"와 같은 구체적인 출처를 제공할 수 있습니다.

특수한 상황을 위한 특수 마커도 정의되었습니다. ERROR_PAGE_MARKER는 "--- [오류페이지] ---" 형식으로, 변환 중 오류가 발생한 페이지를 표시합니다. EMPTY_PAGE_MARKER는 "--- [빈페이지] ---" 형식으로, 텍스트가 없거나 임계값 이하의 텍스트만 포함된 페이지를 표시합니다. 이러한 특수 마커가 있는 페이지는 임베딩 단계에서 자동으로 건너뛰어 불필요한 처리를 방지합니다.

페이지 마커의 삽입 위치는 신중하게 결정되어야 합니다. 마커는 각 페이지 내용의 시작 부분에 삽입되며, 페이지 내용의 일부로 간주되지 않도록 전후에 개행이 추가됩니다. 예를 들어 페이지 5의 내용이 "사업 개요"로 시작한다면, 최종 텍스트는 "--- 페이지 5 ---\n\n사업 개요"와 같은 형태가 됩니다.

Config 설정에서 MARKER_DUMP_ENABLED가 True로 설정되면 페이지 마커가 삽입된 전체 텍스트가 data/markers 디렉토리에 별도 파일로 저장됩니다. 이는 디버깅과 검증을 위한 기능으로, 개발자가 마커 삽입이 올바르게 수행되었는지 시각적으로 확인할 수 있게 합니다.

### 2.3. 데이터베이스 설계

#### 2.3.1. documents_db 스키마

문서 메타데이터를 체계적으로 관리하기 위해 SQLite 데이터베이스를 사용하는 DocumentsDB 클래스가 설계되었습니다. SQLite는 파일 기반의 경량 데이터베이스로, 별도의 서버 설치 없이 애플리케이션에 내장하여 사용할 수 있어 프로토타입 개발에 적합합니다.

TB_DOCUMENTS 테이블은 문서 정보를 저장하는 핵심 테이블입니다. 스키마는 다음과 같이 구성됩니다.

file_hash 컬럼은 TEXT 타입의 기본 키로 정의됩니다. SHA-256 알고리즘으로 생성된 64자리 16진수 문자열이 저장되며, 각 문서를 고유하게 식별합니다. PRIMARY KEY 제약을 통해 동일한 해시값이 중복 저장되는 것을 방지하며, 이는 앞서 설명한 중복 검사 메커니즘의 기반이 됩니다.

chunk_index 컬럼은 INTEGER 타입으로 0 ~ 4 값을 가집니다. 문서 파일을 처리하여 생성한 마크다운을 text_content에 저장할 때 한 칸에 저장하는 양에 제한이 있어서, DB에 저장하기 전에 5조각으로 분할하여 저장하는 데 사용하는 인덱스입니다. file_hash를 호출하면 해당 인덱스에 맞게 text_content가 연결되어 전체 내용이 반환되도록 메서드를 구현하였습니다.

file_name 컬럼은 TEXT 타입으로 NOT NULL 제약이 적용됩니다. 원본 파일의 이름을 저장하며, UNIQUE 제약을 통해 동일한 파일명이 중복되지 않도록 합니다. 이는 파일명 기반 검색을 지원하고, 사용자에게 친숙한 파일 식별자를 제공합니다.

total_pages 컬럼은 INTEGER 타입으로 NOT NULL 제약이 적용됩니다. 원본 PDF의 총 페이지 수를 저장하며, 기본값은 0입니다. 이 정보는 문서의 규모를 파악하고, 처리 시간을 예측하는 데 활용됩니다.

file_size 컬럼은 INTEGER 타입으로 NOT NULL 제약이 적용됩니다. 원본 파일의 크기를 바이트 단위로 저장하며, 기본값은 0입니다. 파일 크기는 디스크 공간 관리와 통계 생성에 사용됩니다.

text_content 컬럼은 TEXT 타입으로 변환된 Markdown 텍스트의 전체 내용을 저장합니다. 이 컬럼은 NULL을 허용하며, 대용량 텍스트를 저장할 수 있도록 설계되었습니다. SQLite의 TEXT 타입은 최대 2GB까지 저장할 수 있어 대부분의 문서를 충분히 수용합니다.

created_at 컬럼은 TIMESTAMP 타입으로 레코드가 생성된 시각을 KST 기준으로 저장합니다. 기본값은 CURRENT_TIMESTAMP에 9시간을 더한 값으로 설정되어 한국 시간대를 반영합니다.

updated_at 컬럼은 TIMESTAMP 타입으로 레코드가 마지막으로 수정된 시각을 KST 기준으로 저장합니다. 기본값은 created_at과 동일하게 설정되며, 문서가 재처리될 때마다 갱신됩니다.

이러한 스키마 설계는 문서 관리의 기본 요구사항을 충족하면서도 향후 확장 가능성을 고려하였습니다. 예를 들어 발주기관, 사업 분류, 입찰 마감일 등의 추가 메타데이터가 필요해지면 새로운 컬럼을 추가하는 방식으로 쉽게 확장할 수 있습니다.

#### 2.3.2. 파일 메타데이터 관리

DocumentsDB 클래스는 파일 메타데이터에 대한 CRUD(Create, Read, Update, Delete) 작업을 추상화된 인터페이스로 제공합니다. 이를 통해 상위 계층의 코드는 SQL 쿼리의 세부 사항을 알 필요 없이 직관적인 메서드 호출만으로 데이터를 관리할 수 있습니다.

insert_text_content 메서드는 새로운 문서 정보를 데이터베이스에 삽입합니다. 이 메서드는 file_name, file_hash, total_pages, file_size, text_content, chunk_index를 파라미터로 받아 TB_DOCUMENTS 테이블에 INSERT 쿼리를 실행합니다. 파일명이 비어 있거나 None인 경우 ValueError 예외를 발생시켜 잘못된 데이터가 저장되는 것을 방지합니다. 중복된 file_hash나 file_name이 있는 경우 SQLite의 UNIQUE 제약 위반 예외가 발생하며, 이는 호출자에게 전파되어 적절한 오류 처리를 할 수 있게 합니다.

get_document_by_hash 메서드는 file_hash를 기준으로 문서 정보를 조회합니다. 해시값이 일치하는 레코드가 있으면 딕셔너리 형태로 반환하고, 없으면 None을 반환합니다. 이 메서드는 중복 검사 단계에서 핵심적으로 사용되며, 인덱스를 통한 빠른 조회가 가능합니다. 이 메서드에서 text_content를 chunk_index 순서대로 합친 뒤에 chunk_index는 제거하여 반환합니다.

get_documents_all 메서드는 데이터베이스의 모든 문서를 조회하여 리스트로 반환합니다. 이는 전체 문서 목록을 UI에 표시하거나, 일괄 처리를 수행할 때 사용됩니다. 대량의 문서가 저장된 경우 메모리 사용량이 증가할 수 있으므로, 향후에는 페이지네이션이나 제너레이터 기반 접근으로 개선할 수 있습니다.

get_document_stats 메서드는 데이터베이스의 통계 정보를 계산하여 반환합니다. 총 파일 수, 총 페이지 수, 총 파일 크기(바이트 및 메가바이트 단위)를 포함하는 딕셔너리를 반환합니다. 이 정보는 시스템 모니터링과 용량 계획에 활용할 수 있습니다.

search_documents 메서드는 파일명 또는 file_hash로 문서를 검색합니다. search_type 파라미터를 통해 검색 방식을 지정할 수 있으며, 'auto' 모드에서는 검색어가 64자리 16진수 문자열이면 해시 검색을, 그렇지 않으면 파일명 LIKE 검색을 수행합니다. 이를 통해 사용자는 해시값이나 파일명 중 편한 방식으로 문서를 찾을 수 있습니다.

모든 데이터베이스 작업은 context manager 패턴을 사용하여 트랜잭션을 관리합니다. _get_connection 메서드는 SQLite 연결 객체를 생성하고 row_factory를 sqlite3.Row로 설정하여 결과를 딕셔너리처럼 접근할 수 있게 합니다. with 문을 통해 자동으로 커밋되고 연결이 종료되므로, 리소스 누수를 방지하고 코드의 안전성을 높입니다.

#### 2.3.3. 재현성 보장 메커니즘

RAG 시스템의 품질을 평가하고 개선하기 위해서는 동일한 입력에 대해 동일한 결과를 재현할 수 있어야 합니다. 이를 위해 파일 해시 기반의 재현성 보장 메커니즘이 설계되었습니다.

file_hash는 문서 내용의 지문으로 작용합니다. 동일한 파일은 언제 어디서 처리하더라도 항상 같은 해시값을 생성합니다. 이 해시값을 기본 키로 사용함으로써, 동일한 문서는 시스템에 단 한 번만 저장되고 처리됩니다.

Markdown 변환 결과는 text_content 컬럼에 저장되어 원본 데이터로서 보존됩니다. 이후 임베딩 처리 과정에서 전처리 방식이나 청킹 전략이 변경되더라도, text_content에서 다시 읽어와서 재처리할 수 있습니다. 이는 실험적인 개선을 시도할 때 매번 PDF 변환을 다시 수행할 필요가 없게 하여 개발 속도를 크게 향상시킵니다.

created_at과 updated_at 타임스탬프는 문서의 생명주기를 추적합니다. 특정 시점 이후에 추가되거나 변경된 문서만을 선택적으로 처리할 수 있어, 증분 업데이트가 가능합니다. 예를 들어 매일 밤 신규 공고를 수집하는 배치 작업에서는 어제 이후 created_at을 가진 문서만을 임베딩 처리하면 됩니다.

데이터베이스 버전 관리를 위해 PRAGMA user_version을 사용합니다. 스키마가 변경될 때마다 버전 번호를 증가시키고, 데이터베이스 초기화 시 현재 버전을 확인하여 필요한 경우 마이그레이션을 수행합니다. 현재는 버전 1이지만, 향후 컬럼 추가나 인덱스 변경이 필요할 때 이 메커니즘을 통해 기존 데이터를 보존하면서 스키마를 업데이트할 수 있습니다.

### 2.4. 중복 데이터 분석 결과

#### 2.4.1. 해시 기반 중복 검출

프로젝트 진행 중 수집한 정부나라장터 공고 문서에 대해 해시 기반 중복 검사를 수행한 결과, 두 건의 중복 사례가 발견되었습니다. 이는 전체 수집 문서의 일부에 해당하지만, 중복 검사 메커니즘의 필요성을 입증하는 중요한 사례입니다.

첫 번째 중복 사례는 해시값 20cdb1e78194ab2496ca941e3c9a4a2b74558dd007535964517584cab67be0a7을 가진 문서입니다. 저장된 파일은 "BioIN의료기기산업 종합정보시스템(정보관리기관) 기능개선 사업(2차).hwp"이며, 스킵된 파일은 "한국보건산업진흥원 의료기기산업 종합정보시스템(정보관리기관) 기능.hwp"입니다. 두 파일의 이름은 다르지만 내용이 완전히 동일함을 해시값을 통해 확인할 수 있습니다.

두 번째 중복 사례는 해시값 fe07779f264abfd5f420f332adb65779a33142a5506581345f5cbe4803be53e8을 가진 문서입니다. 저장된 파일은 "국가과학기술지식정보서비스 통합정보시스템 고도화 용역.hwp"이며, 스킵된 파일은 "한국한의학연구원 통합정보시스템 고도화 용역.hwp"입니다.

이러한 중복은 정부나라장터의 공고 특성을 반영합니다. 유사한 시스템 구축 사업이 여러 기관에 발주되면서, 제안요청서의 상당 부분이 동일하거나 매우 유사한 경우가 발생합니다. 특히 표준화된 시스템이나 공통 플랫폼의 경우 기술 요구사항이나 과업 범위가 거의 동일하여 문서 전체가 중복되는 경우도 있습니다.

중복 검출 메커니즘이 없었다면 이러한 문서들이 각각 처리되어 다음과 같은 문제가 발생했을 것입니다.
1. 불필요한 저장 공간 사용입니다. 동일한 내용의 Markdown 텍스트가 두 번 저장되어 데이터베이스 크기가 증가합니다.
2. 중복된 임베딩 벡터 생성입니다. 동일한 내용에 대해 벡터 임베딩이 두 번 생성되어 FAISS 인덱스 크기가 불필요하게 커지고 검색 성능이 저하됩니다.
3. 검색 결과의 중복입니다. 사용자가 질의를 입력하면 동일한 내용이 서로 다른 파일명으로 여러 번 나타나 혼란을 야기합니다.

해시 기반 중복 검사를 통해 이러한 문제를 사전에 방지할 수 있었습니다. 첫 번째로 발견된 파일은 정상적으로 처리되어 데이터베이스에 저장되고, 이후 동일한 해시값을 가진 파일이 입력되면 자동으로 건너뛰어집니다. 사용자에게는 "이미 처리된 문서입니다. (기존 파일: BioIN의료기기산업...)"와 같은 메시지가 표시되어 중복 처리가 발생했음을 알립니다.

#### 2.4.2. 파일 매핑 관계

중복으로 판단된 파일 쌍의 메타데이터를 분석하면 흥미로운 패턴을 발견할 수 있습니다.

첫 번째 쌍인 BioIN 문서의 경우 메타데이터는 다음과 같습니다. 제목은 "제안요청서_서울바이오허브"이고, 작성자는 "김지승"입니다. 최초 작성일은 2017년 4월 18일이며, 최종 저장일은 2024년 8월 20일입니다. 이는 오래전에 작성된 문서가 최근에 수정되었음을 나타냅니다.

두 번째 쌍인 한의학연구원 문서의 경우 메타데이터는 다음과 같습니다. 제목은 "한의학연구원 통합정보시스템 고도화 용역 제안요청서"이고, 작성자는 "한국한의학연구원"입니다. 최초 작성일은 2005년 2월 28일이며, 최종 저장일은 2024년 5월 30일입니다.

이러한 메타데이터 분석을 통해 중복 파일의 성격을 이해할 수 있습니다. 대부분의 경우 템플릿이나 표준 제안요청서를 기반으로 여러 발주기관이 유사한 문서를 작성합니다. 이 때, 파일명이나 일부 메타데이터는 다르지만 핵심 내용은 동일하므로, 동일한 해시값을 가집니다.

파일 매핑 정보는 시스템 관리와 문서 추적에 유용합니다. 중복 검사 로그에는 저장된 파일과 스킵된 파일의 매핑 관계가 기록되어, 나중에 특정 파일을 검색할 때 실제로 저장된 버전을 찾을 수 있습니다. 예를 들어 사용자가 "한국보건산업진흥원 의료기기산업..." 파일을 찾는다면, 시스템은 이 파일이 중복으로 스킵되었고 실제 내용은 "BioIN의료기기산업..." 파일에 있음을 안내할 수 있습니다.

#### 2.4.3. 데이터 품질 개선 효과

중복 데이터 분석과 제거를 통해 전체 시스템의 품질을 다각도로 개선할 수 있습니다.

저장 공간 측면에서는 직접적인 효과가 나타납니다. 중복 문서가 제거되어 데이터베이스 크기가 감소하고, 백업 및 복구 시간이 단축됩니다. 본 프로젝트의 원본 데이터에서 발견된 두 건의 중복을 처리하여 전체 162 MB 중 1.6 MB (1%)의 저장 공간을 절약할 수 있었습니다. 대규모 시스템에서는 다량의 중복이 발견될 수 있으며, 이 경우 기가바이트 단위의 저장 공간 절약 효과가 발생할 것으로 예상합니다.

임베딩 처리에서도 상당한 비용 절감 효과가 있습니다. OpenAI API는 토큰 수에 비례하여 과금되므로, 동일한 문서를 중복 처리하는 것은 직접적인 비용 증가로 이어집니다. text-embedding-3-small 모델의 경우 백만 토큰당 0.02달러가 부과되는데, 중복 문서 하나가 평균 10만 토큰이라면 문서당 0.002달러의 비용이 절감됩니다.

검색 품질 측면에서는 검색 속도와 다양성 면에서 개선이 이루어집니다. 중복 문서가 제거되어 FAISS 인덱스의 크기가 감소하고 검색 속도가 향상됩니다. 중복 내용이 제거되어 동일한 내용이 여러 번 나타나지 않으므로, 상위 k개 결과가 서로 다른 문서에서 나온 정보를 포함할 가능성이 높아집니다.

사용자 경험 측면에서는 혼란을 방지하고 신뢰성을 향상시킬 수 있습니다. 검색 결과에 동일한 내용이 서로 다른 파일명으로 여러 번 나타나면 사용자는 어느 것이 올바른 출처인지 판단하기 어렵습니다. 중복 제거를 통해 각 정보는 단 하나의 명확한 출처를 가지게 되어 사용자의 혼란을 방지하고 출처의 신뢰성이 향상됩니다.

시스템 유지보수 측면에서도 이점이 있습니다. 중복이 없는 깔끔한 데이터베이스는 분석과 디버깅을 용이하게 합니다. 특정 문서에 문제가 발견되었을 때 중복된 버전이 없으므로 단 한 곳만 수정하면 됩니다. 또한 통계 정보가 정확해져서 시스템의 실제 용량과 성능을 올바르게 평가할 수 있습니다.

### 2.5. (기능 추가) 나라장터의 특정 기간의 공고에 있는 첨부 문서를 처리하여 DB에 추가

공공데이터포털(data.go.kr)에서 조달청_나라장터 입찰공고정보 서비스를 신청하고 API Key(DATA_GO_KR_SERVICE_KEY로 명명)를 받아서 저희 팀이 만든 APP UI에서 SERVICE_KEY, 시작 일자와 종료 일자를 입력하면 해당 기간의 입찰 공고 정보를 조회하여 파일을 다운로드 받아서 위의 전처리를 거쳐서 DB에 저장하여 추가하는 내용을 구현하였습니다.

#### 2.5.1. Service Key 검증 및 입찰 공고 정보 조회

APP UI 상에서 Service Key와 검색할 시작 일자와 종료 일자를 선택하며 날짜를 처리하여 api_url을 생성하여 API에 요청하면 해당 기간의 공고 정보를 응답으로 받아올 수 있게 구성하였습니다. 공고 정보를 받을 때 10초의 timeout을 설정하여 네트워크 문제 등으로 접속이 원활하지 않을 때 종료되도록 설정하였습니다.

api_url에는 조달청_나라장터 입찰공고 서비스의 url인 "https://apis.data.go.kr/1230000/ad/BidPublicInfoService/"과 입찰공고 서비스 참고 자료를 바탕으로 "입찰공고목록 정보에 대한 공사조회" 항목인 "getBidPblancListInfoCnstwk"이 포함되어 페이지 당 100건이 출력되도록 설정하였습니다.

#### 2.5.2. API 응답 중 첨부 문서를 받아서 DB에 저장

API 응답을 얻으면 항목 리스트에서 첨부파일 URL만을 추출하여 중복을 제거하고 유효한 항목만을 선별하여 tempfile.mkdtemp() 함수를 통해 중복되지 않는 임시 디렉토리를 생성하여 파일을 저장합니다.

다운로드 요청 시 타임아웃 30초와 HTTP 또는 원인 미상의 오류 발생 시 다음 파일을 진행하도록 하였고, URL 상의 유효하지 않은 문자를 정리하고 경로 길이가 길어지는 경우를 대비한 절대 경로 변환도 포함하였습니다.

다운로드가 완료되면 이번 프로젝트에서 구현한 HWP, PDF 처리 모듈을 이용하여 다운로드한 파일을 마크다운 변환하여 File_hash 기준으로 중복되지 않은 내용을 DB에 추가합니다. DB에 저장이 완료된 뒤에는 임시 디렉토리에 다운로드 받았던 파일들을 제거합니다.