---
layout: default
title: "[중급프로젝트] RAG 기반 정부나라장터 입찰공고 분석 시스템 - LLM 기반 정보 추출 및 요약"
description: "[중급프로젝트] RAG 기반 정부나라장터 입찰공고 분석 시스템 - LLM 기반 정보 추출 및 요약"
date: 2025-11-25
cache-control: no-cache
expires: 0
pragma: no-cache
author: "이민규 (AI 리서처)"
---

## 4. LLM 기반 정보 추출 및 요약 (이민규)

### 4.1. LLM 프로세서 설계

#### 4.1.1. ChatOpenAI 모델 설정

LLM Processor는 RAG 시스템의 최종 단계로서 검색된 문서 청크를 바탕으로 사용자의 질문에 대한 자연어 답변을 생성합니다. 본 시스템에서는 OpenAI의 ChatGPT 시리즈 모델을 활용하며, LangChain의 ChatOpenAI 클래스를 통해 통합된 인터페이스를 제공합니다.

LLMProcessor 클래스는 초기화 단계에서 모델명과 생성 파라미터를 설정합니다. 모델명은 Config의 OPENAI_MODEL 설정에서 가져오며, 기본값은 gpt-5-mini입니다. gpt-5-mini는 2025년 초에 공개된 최신 모델로 이전 세대 모델 대비 추론 속도가 빠르고 비용이 낮으면서도 우수한 성능을 보입니다. 시스템 요구사항이나 비용 제약에 따라 gpt-4o, gpt-4-turbo 등 다른 모델로 쉽게 전환할 수 있도록 설계되었습니다.

Temperature 파라미터는 생성된 텍스트의 창의성과 일관성을 조절하는 중요한 설정입니다. 0에 가까울수록 결정론적이고 일관된 답변을 생성하며, 2에 가까울수록 창의적이지만 예측하기 어려운 답변을 생성합니다. RAG 시스템에서는 검색된 문서를 바탕으로 사실적인 정보를 전달해야 하므로, Config의 OPENAI_TEMPERATURE는 기본값 0.0으로 설정되어 있습니다. 이를 통해 동일한 질문과 컨텍스트에 대해 항상 일관된 답변을 제공하여 사용자 경험을 예측 가능하게 만듭니다.

모델별 특수 처리가 구현되어 있어 각 모델의 제약사항을 자동으로 관리합니다. gpt-5-mini 모델의 경우 temperature 파라미터로 0.0을 지원하지 않으므로, 생성자에서 자동으로 1.0으로 조정하고 경고 메시지를 로깅합니다. 이러한 처리를 통해 모델 변경 시 발생할 수 있는 API 오류를 사전에 방지합니다.

API 키 관리는 보안과 편의성을 모두 고려하여 설계되었습니다. 우선순위는 메서드 파라미터로 전달된 API 키, 환경 변수 OPENAI_API_KEY, Config 설정 순서입니다. 이를 통해 개발 환경에서는 환경 변수를 사용하고, 프로덕션 환경에서는 파라미터로 전달하는 등 유연한 운영이 가능합니다. API 키가 설정되지 않았거나 빈 문자열인 경우 명확한 오류 메시지와 함께 ValueError 예외를 발생시켜 문제를 조기에 발견할 수 있도록 합니다.

LangChain의 ChatOpenAI 클래스는 OpenAI API 호출을 추상화하여 재시도 로직, 스트리밍 응답, 토큰 카운팅 등의 기능을 자동으로 처리합니다. 네트워크 오류나 일시적인 API 장애 발생 시 자동으로 재시도하며, 지수 백오프 전략을 사용하여 서버 부하를 최소화합니다. 최대 재시도 횟수는 3회로 제한되어 있으며, 모든 재시도가 실패하면 예외를 발생시켜 상위 계층에서 적절히 처리할 수 있도록 합니다.

#### 4.1.2. Temperature 및 파라미터 최적화

LLM의 출력 품질은 다양한 생성 파라미터에 의해 결정됩니다. 본 시스템에서는 RAG 애플리케이션의 특성에 맞게 파라미터를 최적화하였습니다.

Temperature는 앞서 설명한 바와 같이 0.0으로 설정하여 결정론적 답변을 보장합니다. RAG 시스템의 목표는 창의적인 이야기를 생성하는 것이 아니라 검색된 문서를 바탕으로 정확한 정보를 전달하는 것이므로, 낮은 temperature 값이 적합합니다. 실험 결과 temperature 0.0과 0.3을 비교했을 때, 0.0에서 문서 내용을 더 충실하게 반영하고 불필요한 추론을 최소화하는 경향을 보였습니다.

Max_tokens 파라미터는 생성할 최대 토큰 수를 지정합니다. 너무 작게 설정하면 답변이 중간에 잘릴 수 있고, 너무 크게 설정하면 불필요하게 긴 답변이 생성되어 응답 시간과 비용이 증가합니다. 본 시스템에서는 대부분의 질문에 충분한 답변을 제공할 수 있는 50000 토큰으로 설정하였습니다. 입찰 공고 문서에 대한 질문은 단순한 사실 확인부터 복잡한 요구사항 분석까지 다양한 범위를 가지므로, 충분한 여유를 두는 것이 중요합니다.

모델별로 지원하는 파라미터가 다르므로 이를 자동으로 처리하는 로직이 구현되어 있습니다. GPT-5 시리즈, GPT-4.1 시리즈, O1 시리즈 모델은 max_completion_tokens 파라미터를 사용하며 temperature 파라미터를 지원하지 않습니다. 이러한 모델에 대해서는 max_completion_tokens를 50000으로 설정하고 temperature 파라미터를 전달하지 않습니다. 반면 GPT-4o, GPT-4-turbo 등 이전 세대 모델은 temperature와 max_tokens 파라미터를 모두 지원합니다. generate_response 메서드는 모델명을 확인하여 적절한 파라미터 조합을 자동으로 선택합니다.

Top_p 파라미터는 nucleus sampling을 제어하는데, 누적 확률이 이 값에 도달할 때까지의 토큰만 고려합니다. 본 시스템에서는 기본값인 1.0을 사용하여 모든 토큰을 고려 대상에 포함시킵니다. Temperature가 0.0이므로 top_p 값은 실질적인 영향을 미치지 않지만, 향후 temperature를 조정할 경우를 대비하여 명시적으로 설정하였습니다.

Presence_penalty와 frequency_penalty는 반복을 억제하는 파라미터입니다. Presence_penalty는 이미 등장한 토큰의 재등장을 페널티 주고, frequency_penalty는 등장 빈도에 비례하여 페널티를 부여합니다. RAG 시스템에서는 문서 내용을 충실히 반영해야 하므로 이러한 페널티를 적용하지 않으며, 기본값인 0.0을 유지합니다. 만약 답변이 지나치게 반복적이라는 피드백이 있을 경우 이 값들을 조정하여 개선할 수 있습니다.

Stop 시퀀스는 특정 문자열이 생성되면 즉시 생성을 중단하는 기능입니다. 본 시스템에서는 사용하지 않지만, 향후 특정 형식의 답변을 강제하고 싶을 때 활용할 수 있습니다. 예를 들어 JSON 형식의 답변을 요구하는 경우 닫는 중괄호 다음에 생성을 중단하도록 설정할 수 있습니다.

#### 4.1.3. gpt-5-mini 호환성 처리

gpt-5-mini는 OpenAI의 최신 소형 모델로 빠른 응답 속도와 낮은 비용을 제공하지만, 이전 모델과 다른 파라미터 제약을 가지고 있습니다. 본 시스템에서는 이러한 차이를 자동으로 처리하여 개발자가 의식하지 않아도 되도록 구현하였습니다.

가장 중요한 제약은 temperature 파라미터입니다. gpt-5-mini는 temperature 값으로 0.0을 지원하지 않으며, 최소값은 0.3입니다. 이는 OpenAI가 해당 모델의 특성상 완전히 결정론적인 출력이 적합하지 않다고 판단했기 때문입니다. LLMProcessor의 생성자에서는 모델명을 확인하여 gpt-5-mini이고 temperature가 0.0으로 설정된 경우, 자동으로 1.0으로 변경하고 경고 로그를 출력합니다. 이를 통해 API 오류를 사전에 방지하면서도 사용자에게 변경 사실을 알립니다.

Max_completion_tokens 파라미터 사용도 gpt-5-mini의 특징입니다. 이전 모델들은 max_tokens 파라미터를 사용했지만, GPT-5 시리즈부터는 max_completion_tokens로 명칭이 변경되었습니다. 이는 입력 토큰과 출력 토큰을 명확히 구분하기 위한 변경입니다. generate_response 메서드는 모델명에 gpt-5나 gpt-4.1, o1 등이 포함되어 있는지 확인하여 적절한 파라미터를 사용합니다.

컨텍스트 윈도우 크기도 고려해야 합니다. gpt-5-mini는 최대 128K 토큰의 컨텍스트를 지원하지만, 실제 사용 가능한 토큰 수는 입력과 출력의 합으로 제한됩니다. 본 시스템에서는 검색된 청크와 프롬프트를 합쳐도 대부분 10K 토큰 이내이므로 문제가 없지만, 향후 더 많은 컨텍스트를 포함하고자 할 경우 입력 토큰 수를 모니터링하고 필요시 청크 수를 제한해야 합니다.

응답 시간 특성도 다릅니다. gpt-5-mini는 더 작은 모델이므로 gpt-4o나 gpt-4-turbo에 비해 응답이 빠릅니다. 평균적으로 2초에서 5초 내에 답변을 생성하여 실시간 대화에 적합합니다. 반면 복잡한 추론이 필요한 질문에서는 큰 모델 대비 성능이 낮을 수 있으므로, 사용 사례에 따라 적절한 모델을 선택해야 합니다.

비용 효율성은 gpt-5-mini의 가장 큰 장점입니다. 입력 백만 토큰당 0.075달러, 출력 백만 토큰당 0.3달러로 gpt-4o의 입력 2.5달러, 출력 10달러 대비 크게 저렴합니다. 프로토타입 단계나 대용량 처리가 필요한 경우 비용 절감 효과가 매우 큽니다. 본 시스템에서는 기본 모델로 gpt-5-mini를 사용하되, 필요시 Config 설정만 변경하여 더 강력한 모델로 전환할 수 있도록 유연성을 확보하였습니다.

### 4.2. 프롬프트 엔지니어링

#### 4.2.1. RAG 프롬프트 템플릿 설계

프롬프트는 LLM이 어떤 작업을 수행해야 하는지 지시하는 핵심 요소입니다. RAG 시스템에서는 검색된 문서를 컨텍스트로 제공하고, 이를 바탕으로 질문에 답변하도록 유도하는 프롬프트 구조가 필요합니다.

본 시스템의 기본 프롬프트 템플릿은 명확한 구조를 가지고 있습니다. 먼저 시스템 역할을 정의하는 지시문이 포함됩니다. "다음 컨텍스트를 참고하여 질문에 답변해주세요"와 같이 LLM의 역할을 명시하여 모델이 작업의 목적을 이해하도록 합니다. 이어서 컨텍스트 섹션이 제공되며, 여기에는 검색된 청크들이 포맷된 형태로 삽입됩니다. 각 청크는 출처 정보와 함께 제공되어 답변 생성 시 참고할 수 있습니다. 질문 섹션에는 사용자의 원래 질의가 그대로 삽입됩니다. 마지막으로 답변을 유도하는 부분에서는 "답변:"과 같은 명시적 라벨을 사용하여 LLM이 즉시 답변을 시작하도록 합니다.

프롬프트 템플릿은 src/llm/prompts 에 저장되어 있으며, 여러 템플릿을 관리할 수 있습니다. context와 question 두 개의 변수가 정의되어 있으며, 실행 시 실제 값으로 치환됩니다. Python의 format 메서드를 사용하여 간단하게 치환할 수 있어 유지보수가 용이합니다.

PromptLoader 클래스는 프롬프트 템플릿을 외부 파일에서 로드하는 기능을 제공합니다. 이를 통해 프롬프트를 수정할 때 코드를 변경하지 않고 설정 파일만 편집하면 됩니다. 템플릿은 JSON 형식으로 저장되며, 여러 버전의 프롬프트를 관리하고 A/B 테스트를 수행할 수 있습니다. 예를 들어 더 상세한 답변을 유도하는 프롬프트와 간결한 답변을 유도하는 프롬프트를 준비하여 상황에 따라 선택할 수 있습니다.

프롬프트 로딩 우선순위는 다음과 같습니다. 먼저 PromptLoader의 templates 딕셔너리에서 rag_prompt_template 키를 찾고, 없으면 template 키를 찾으며, 그것도 없으면 default 키를 찾습니다. 모든 경로에서 찾지 못하면 Config의 RAG_PROMPT_TEMPLATE를 사용합니다. 이를 통해 다양한 경로로 프롬프트를 제공할 수 있으며, 최종적으로는 항상 유효한 프롬프트가 사용되도록 보장합니다.

#### 4.2.2. 컨텍스트 구성 전략

검색된 청크를 LLM에 제공할 때 어떻게 구성하느냐에 따라 답변 품질이 크게 달라집니다. 본 시스템에서는 두 가지 컨텍스트 구성 방식을 지원합니다.

1. 청크 기반 컨텍스트
- 청크 기반 컨텍스트는 Retrieval의 search 메서드 결과를 사용합니다. 
- 각 청크는 독립적으로 포맷되며, 순서대로 나열됩니다. 
- _build_context_from_chunks 메서드는 청크 리스트를 받아 통일된 형식으로 변환합니다. 
- 각 청크는 문서 번호, 파일명과 함께 제시되어 출처를 명확히 합니다. 예를 들어 첫 번째 청크는 "문서 1: 공고문.pdf"와 같은 헤더와 함께 청크 텍스트가 이어집니다. 
- 청크들은 이중 개행으로 구분되어 시각적으로 분리되며, LLM이 각 청크를 독립적인 정보 단위로 인식할 수 있도록 합니다.

2. 페이지 기반 컨텍스트
- 페이지 기반 컨텍스트는 Retrieval의 search_page 메서드 결과를 사용합니다. 
- 페이지 단위로 병합된 텍스트가 제공되므로 더 넓은 문맥을 포함합니다. 
- _build_context_from_pages 메서드는 페이지 리스트를 받아 각 페이지를 포맷합니다. 
- 페이지 정보에는 파일명, 페이지 번호, 유사도 점수가 포함되어 출처를 더욱 상세히 제공합니다. 예를 들어 "출처 1: 공고문.pdf, 페이지 5, 유사도=0.1234"와 같은 형식입니다. 
- 유사도 점수는 소수점 넷째 자리까지 표시되어 검색 품질을 평가할 수 있는 정보를 제공합니다.

컨텍스트 크기 제한은 max_chunks 파라미터로 제어됩니다. 검색 결과가 많을 경우 모든 청크를 포함하면 컨텍스트가 지나치게 길어져 토큰 한도를 초과하거나 LLM의 집중력이 분산될 수 있습니다. max_chunks가 지정되면 상위 n개의 청크만 포함하여 컨텍스트를 제한합니다. 기본값은 5로 설정되어 있어 대부분의 질문에 충분한 정보를 제공하면서도 관리 가능한 크기를 유지합니다.

컨텍스트가 비어 있는 경우의 처리도 중요합니다. 검색 결과가 없거나 모든 청크가 임계값 이하의 유사도를 가진 경우, Config의 NO_CONTEXT_MESSAGE가 사용됩니다. 이는 "검색된 관련 문서가 없습니다"와 같은 메시지로, LLM이 임의로 답변을 생성하지 않고 정보 부족을 명확히 알리도록 유도합니다. 이를 통해 환각 현상을 방지하고 시스템의 신뢰성을 높입니다.

자동 포맷 감지 기능은 _build_context 메서드에서 구현됩니다. 이 메서드는 retrieved_chunks의 타입을 확인하여 딕셔너리이면서 pages 키를 포함하면 페이지 기반으로, 리스트이면 청크 기반으로 자동 판단합니다. 이를 통해 상위 계층 코드는 검색 방식에 관계없이 동일한 인터페이스로 LLMProcessor를 호출할 수 있습니다.

#### 4.2.3. 한국어 응답 최적화

본 시스템은 한국어 입찰 공고 문서를 대상으로 하므로, 한국어 응답의 자연스러움과 정확성이 매우 중요합니다. OpenAI 모델은 영어에 최적화되어 있지만, 적절한 프롬프트 전략으로 한국어 품질을 크게 향상시킬 수 있습니다.

프롬프트 언어를 한국어로 작성하는 것이 첫 번째 전략입니다. 시스템 지시문, 섹션 라벨, 유도 문구 등을 모두 한국어로 작성하여 LLM이 한국어 모드로 작동하도록 합니다. "다음 컨텍스트를 참고하여 질문에 답변해주세요"와 같이 명확한 한국어 지시문을 제공하면, 영어로 작성된 지시문보다 한국어 답변의 자연스러움이 향상됩니다.

경어 수준 조정도 중요합니다. 입찰 공고는 공식 문서이므로 높임말을 사용하는 것이 적절합니다. 프롬프트에 "존댓말을 사용하여 답변해주세요"와 같은 명시적 지시를 포함할 수 있습니다. 실험 결과 이러한 지시가 있을 때 LLM이 일관되게 존댓말을 사용하며, 격식 있는 어투를 유지하는 것을 확인하였습니다.

용어 일관성은 도메인 특화 용어를 올바르게 사용하도록 하는 것입니다. 입찰 공고에는 제안요청서, 과업지시서, 기술규격서와 같은 전문 용어가 많이 등장합니다. 프롬프트에 "입찰 및 조달 분야의 전문 용어를 정확히 사용하세요"와 같은 지시를 추가하면 용어 오용을 줄일 수 있습니다. 또한 컨텍스트에 포함된 문서 자체가 올바른 용어를 사용하고 있으므로, LLM이 이를 참고하여 일관된 용어를 사용하게 됩니다.

문장 길이와 구조도 고려 대상입니다. 한국어는 영어에 비해 문장이 길고 복잡한 구조를 가질 수 있습니다. 프롬프트에 "간결하고 명확한 문장으로 답변해주세요"와 같은 지시를 포함하여 지나치게 장황한 답변을 방지할 수 있습니다. 또한 "핵심 정보를 먼저 제시하세요"와 같은 지시로 답변의 구조를 개선할 수 있습니다.

숫자와 단위 표기도 한국어 관례를 따라야 합니다. 금액은 "1억 5천만원"과 같이 한국식 단위를 사용하고, 날짜는 "2025년 11월 28일"과 같이 표기합니다. 대부분의 경우 LLM이 컨텍스트의 표기를 따라하므로 자연스럽게 처리되지만, 필요시 프롬프트에 명시적 지시를 추가할 수 있습니다.

출처 표시 방식도 한국어에 맞게 조정되어야 합니다. "이 정보는 공고문.pdf의 5페이지에 있습니다"와 같이 자연스러운 한국어 문장으로 출처를 언급하도록 유도합니다. 프롬프트에 "답변 후 출처 정보를 제공하세요"와 같은 지시를 포함하여 일관된 형식을 유지합니다.

### 4.3. 대화 이력 관리

#### 4.3.1. ChatHistoryDB 스키마

대화형 RAG 시스템에서는 사용자와의 상호작용을 기록하고 관리하는 것이 중요합니다. ChatHistoryDB는 SQLite 데이터베이스를 사용하여 채팅 세션과 메시지를 체계적으로 저장합니다.

데이터베이스는 두 개의 주요 테이블로 구성됩니다. chat_sessions 테이블은 각 대화 세션의 메타데이터를 저장합니다. session_id는 UUID 형식의 기본 키로 각 세션을 고유하게 식별합니다. session_name은 사용자 친화적인 세션 이름으로, 기본값은 현재 시간을 포함한 형식입니다. created_at은 세션이 생성된 시각을, updated_at은 마지막 메시지가 추가된 시각을 기록합니다. is_active는 세션의 활성 상태를 나타내는 불린 값으로, 종료된 대화를 아카이브할 때 사용됩니다.

chat_messages 테이블은 실제 대화 내용을 저장합니다. message_id는 자동 증가하는 정수형 기본 키로 각 메시지를 식별합니다. session_id는 chat_sessions 테이블을 참조하는 외래 키로, CASCADE 삭제 옵션이 설정되어 세션 삭제 시 관련 메시지도 자동으로 삭제됩니다. role은 메시지의 발신자를 나타내며 user 또는 assistant 값을 가집니다. content는 메시지의 실제 텍스트 내용을 저장합니다. retrieved_chunks는 user 메시지에 대해 검색된 청크 정보를 JSON 형식으로 저장하며, assistant 메시지에서는 NULL입니다. timestamp는 메시지가 생성된 시각을 기록합니다.

스키마 설계 시 고려된 요소들이 있습니다. 외래 키 제약을 통해 데이터 무결성을 보장하며, 존재하지 않는 세션에 메시지를 추가할 수 없습니다. 인덱스는 session_id와 timestamp에 자동으로 생성되어 세션별 메시지 조회와 시간 기반 정렬이 빠르게 수행됩니다. JSON 형식의 retrieved_chunks는 유연한 메타데이터 저장을 가능하게 하며, 청크 구조가 변경되어도 스키마 수정 없이 대응할 수 있습니다.

데이터베이스 파일은 Config의 CHAT_HISTORY_DB_PATH에 지정된 경로에 생성되며, 기본값은 data/chat_history.db입니다. SQLite의 장점인 파일 기반 저장 방식 덕분에 별도의 데이터베이스 서버 없이도 안정적으로 작동하며, 백업과 복구가 파일 복사만으로 간단히 수행됩니다.

#### 4.3.2. 세션 기반 대화 관리

세션은 사용자와 시스템 간의 연속된 대화를 논리적으로 그룹화하는 단위입니다. 각 세션은 특정 주제나 작업을 중심으로 진행되며, 여러 세션을 병렬적으로 관리할 수 있습니다.

세션 생성은 ChatHistoryDB의 create_session 메서드를 통해 수행됩니다. UUID를 사용하여 전역적으로 고유한 세션 ID를 생성하므로, 분산 환경에서도 충돌 없이 세션을 식별할 수 있습니다. 세션 이름은 사용자가 지정할 수 있으며, 지정하지 않으면 현재 시각을 포함한 기본 이름이 자동 생성됩니다. 예를 들어 "채팅 세션 2025-11-25 14:30"과 같은 형식입니다. 이를 통해 사용자는 나중에 대화 목록에서 세션을 쉽게 식별할 수 있습니다.

메시지 추가는 add_message 메서드를 통해 이루어집니다. 이 메서드는 세션 ID, 역할, 내용, 검색된 청크를 파라미터로 받아 chat_messages 테이블에 INSERT 쿼리를 실행합니다. 메시지 추가 시 해당 세션의 updated_at도 자동으로 갱신되어 최근 활동 시각을 추적할 수 있습니다. 메서드는 추가된 메시지의 ID를 반환하여 상위 계층에서 필요시 참조할 수 있도록 합니다.

검색된 청크 정보는 retrieved_chunks 파라미터를 통해 전달됩니다. 이는 리스트 형태의 딕셔너리로, 각 딕셔너리는 청크의 메타데이터를 포함합니다. add_message 메서드는 이를 JSON 문자열로 직렬화하여 데이터베이스에 저장합니다. ensure_ascii=False 옵션을 사용하여 한글이 유니코드 이스케이프 없이 그대로 저장되도록 합니다.

세션 목록 조회는 list_sessions 메서드를 통해 가능합니다. 모든 세션을 updated_at 기준 내림차순으로 정렬하여 반환하므로, 가장 최근에 활동한 세션이 먼저 나타납니다. 각 세션의 정보에는 ID, 이름, 생성 시각, 수정 시각, 활성 상태가 포함되어 UI에서 세션 목록을 표시할 수 있습니다.

특정 세션의 메시지 조회는 get_session_messages 메서드를 사용합니다. 세션 ID를 전달하면 해당 세션의 모든 메시지를 시간순으로 정렬하여 반환합니다. 반환된 메시지는 딕셔너리 형태로, role, content, retrieved_chunks, timestamp 등의 정보를 포함합니다. 이를 통해 대화 이력을 UI에 표시하거나 분석에 활용할 수 있습니다.

세션 삭제는 delete_session 메서드를 제공합니다. CASCADE 삭제 옵션 덕분에 세션을 삭제하면 관련된 모든 메시지도 자동으로 삭제되어 데이터 일관성이 유지됩니다. 메서드는 삭제 성공 여부를 불린 값으로 반환하여 호출자가 결과를 확인할 수 있게 합니다.

통계 정보는 get_chat_stats 메서드로 조회할 수 있습니다. 총 세션 수, 활성 세션 수, 총 메시지 수, 사용자 메시지 수, 어시스턴트 메시지 수 등의 정보를 제공하여 시스템 사용 현황을 파악할 수 있습니다. 이러한 통계는 시스템 모니터링이나 사용자 행동 분석에 활용됩니다.

#### 4.3.3. 검색 청크 메타데이터 저장

대화 이력에 검색 청크 정보를 함께 저장하는 것은 여러 이점을 제공합니다. 답변의 근거를 추적할 수 있고, 검색 품질을 평가할 수 있으며, 디버깅과 개선에 활용할 수 있습니다.

청크 메타데이터 구조는 검색 단계에서 사용된 것과 동일합니다. file_hash, file_name, start_page, end_page, chunk_index, chunk_hash, distance, created_at 등의 필드가 포함됩니다. 이 정보는 리스트 형태로 여러 청크를 포함할 수 있으며, 각 청크는 독립적인 딕셔너리로 표현됩니다.

JSON 직렬화 과정에서 몇 가지 처리가 필요합니다. LLMProcessor의 generate_response 메서드는 retrieved_chunks를 데이터베이스에 저장하기 전에 _convert_tuple_keys_to_str 메서드를 호출하여 튜플 키를 문자열로 변환합니다. Python 딕셔너리는 튜플을 키로 사용할 수 있지만, JSON은 문자열 키만 지원하므로 이러한 변환이 필요합니다. 또한 _add_file_hash 메서드를 호출하여 각 청크에 file_hash 필드가 누락된 경우 자동으로 추가합니다.

file_hash 패치는 중요한 기능입니다. 일부 검색 결과에서 file_hash가 누락될 수 있는데, 이 경우 청크 텍스트의 MD5 해시를 계산하여 file_hash로 사용합니다. 이를 통해 데이터 일관성을 유지하고 향후 분석에서 파일별 그룹화가 가능하도록 합니다.

저장된 청크 정보는 다양한 용도로 활용됩니다. 사용자가 특정 답변에 대해 더 자세한 정보를 요청하면, 저장된 청크 정보를 바탕으로 원본 문서의 정확한 위치를 안내할 수 있습니다. 검색 품질 평가 시에는 저장된 distance 값을 분석하여 유사도 분포를 파악하고, 임계값 조정에 활용할 수 있습니다. A/B 테스트에서는 서로 다른 검색 전략의 결과를 비교하여 어느 것이 더 관련성 높은 청크를 반환하는지 평가할 수 있습니다.

청크 정보의 크기가 클 수 있다는 점도 고려되었습니다. 각 청크는 수백 바이트에서 수 킬로바이트의 메타데이터를 포함할 수 있으며, 세션당 수십 개의 메시지가 저장될 수 있습니다. SQLite의 TEXT 타입은 최대 2GB를 지원하므로 용량 문제는 없지만, 매우 긴 대화의 경우 데이터베이스 크기가 증가할 수 있습니다. 향후 오래된 세션을 아카이브하거나 청크 정보를 별도 테이블로 분리하는 등의 최적화를 고려할 수 있습니다.

### 4.4. 메모리 및 요약

#### 4.4.1. ConversationSummaryMemory 구현

긴 대화가 진행될수록 전체 대화 이력을 LLM 컨텍스트에 포함하는 것이 어려워집니다. 토큰 한도를 초과하거나 응답 시간이 길어지는 문제가 발생합니다. ConversationSummaryMemory는 이러한 문제를 해결하기 위해 대화 이력을 요약하여 관리합니다.

LangChain의 ConversationSummaryMemory 클래스는 대화가 진행됨에 따라 자동으로 요약을 생성합니다. 초기에는 전체 대화 이력을 유지하다가, 토큰 수가 설정된 임계값을 초과하면 오래된 대화를 요약으로 대체합니다. 새로운 메시지는 그대로 유지되어 최근 컨텍스트는 보존됩니다.

LLMProcessor의 생성자에서 ConversationSummaryMemory를 초기화합니다. llm 파라미터로 ChatOpenAI 인스턴스를 전달하여 요약 생성에 사용할 모델을 지정합니다. max_token_limit 파라미터는 요약 트리거 임계값으로, 기본값은 1500 토큰입니다. 대화 이력이 이 값을 초과하면 자동으로 요약이 생성됩니다. prompt 파라미터로 커스텀 요약 프롬프트를 전달하여 요약 스타일을 제어할 수 있습니다.

요약 프로세스는 다음과 같이 진행됩니다. 새로운 메시지가 추가될 때마다 ConversationSummaryMemory는 현재 대화 이력의 토큰 수를 계산합니다. 토큰 수가 max_token_limit를 초과하면 가장 오래된 메시지들을 선택하여 요약 대상으로 지정합니다. 선택된 메시지들을 요약 프롬프트와 함께 LLM에 전달하여 요약문을 생성합니다. 생성된 요약문으로 오래된 메시지들을 대체하여 메모리에 저장합니다. 이 과정은 비동기적으로 수행되어 사용자 경험에 영향을 주지 않습니다.

요약의 품질은 프롬프트 설계에 크게 의존합니다. 단순히 "요약하세요"라고 지시하는 것보다 구체적인 지침을 제공하는 것이 효과적입니다. 본 시스템에서는 한국어 요약 프롬프트를 사용하여 자연스러운 한국어 요약을 생성합니다. 프롬프트에는 기존 요약과 새로운 대화를 모두 포함하여 누적적인 요약이 이루어지도록 합니다.

요약 메모리의 장점은 여러 가지입니다. 토큰 사용량을 제한하여 비용을 절감하고 응답 속도를 유지합니다. 긴 대화에서도 주요 내용을 보존하여 컨텍스트의 연속성을 유지합니다. 자동으로 관리되므로 개발자가 수동으로 메모리를 정리할 필요가 없습니다. 요약은 LLM이 생성하므로 중요한 정보를 놓치지 않고 의미를 보존합니다.

#### 4.4.2. 한국어 요약 프롬프트

한국어 대화를 요약할 때는 언어 특성을 고려한 프롬프트 설계가 필요합니다. 본 시스템에서는 한국어에 최적화된 요약 프롬프트를 구현하였습니다.

KOREAN_SUMMARY_PROMPT는 PromptTemplate 객체로 정의되어 있으며, summary와 new_lines 두 개의 입력 변수를 가집니다. summary는 기존 요약문을, new_lines는 새로 추가될 대화 내용을 나타냅니다. 템플릿은 명확한 구조로 작성되어 있습니다. 먼저 "기존 대화 요약:"이라는 라벨과 함께 기존 요약을 제시합니다. 이어서 "새로운 대화:"라는 라벨과 함께 새로운 내용을 제시합니다. 마지막으로 "위 내용을 바탕으로 한국어로 업데이트된 대화 요약을 작성하세요"라는 지시문으로 작업을 명확히 합니다.

한국어 특성을 반영한 지시가 포함되어 있습니다. "한국어로"라는 명시적 언어 지정으로 영어 혼입을 방지합니다. "업데이트된"이라는 표현으로 기존 요약을 개선하는 방향임을 명확히 합니다. "대화 요약"이라는 명확한 산출물 정의로 요약의 성격을 지정합니다.

요약 스타일도 고려됩니다. 프롬프트에 추가 지시를 포함하여 요약의 길이, 형식, 톤을 제어할 수 있습니다. 예를 들어 "간결하게"라는 지시로 장황한 요약을 방지하거나, "핵심 키워드를 포함하여"라는 지시로 검색 가능성을 높일 수 있습니다. 본 시스템에서는 기본적으로 간결하면서도 주요 정보를 보존하는 요약을 목표로 합니다.

요약 결과는 ConversationSummaryMemory의 내부에 저장되며, load_memory_variables 메서드를 통해 조회할 수 있습니다. get_memory_summary 메서드는 이를 래핑하여 현재 요약문을 문자열로 반환합니다. 요약 조회 실패 시에는 예외를 포착하여 기본 메시지를 반환하므로 시스템이 중단되지 않습니다.

#### 4.4.3. 대화 컨텍스트 유지 전략

효과적인 대화형 시스템은 이전 대화를 기억하고 컨텍스트를 유지해야 합니다. 사용자가 "그것은 무엇인가요?"와 같이 이전 메시지를 참조하는 질문을 할 때, 시스템은 무엇을 가리키는지 알아야 합니다.

RunnableWithMessageHistory는 LangChain의 메커니즘으로, 대화 이력을 자동으로 관리합니다. ConversationChain을 래핑하여 각 호출 시 세션별 대화 이력을 주입하고 응답을 저장합니다. LLMProcessor의 생성자에서 이를 초기화하며, 람다 함수를 통해 세션 ID에 따른 메시지 이력을 제공합니다.

InMemoryChatMessageHistory는 대화 이력을 메모리에 저장하는 간단한 구현체입니다. 각 메시지는 HumanMessage 또는 AIMessage 객체로 저장되며, 시간순으로 정렬됩니다. 메모리 기반이므로 프로세스가 종료되면 이력이 사라지지만, 단일 세션 동안의 대화에는 충분합니다. 영구 저장이 필요한 경우 ChatHistoryDB의 데이터를 활용할 수 있습니다.

세션 ID 기반 격리가 구현되어 있습니다. conversation_chain의 invoke 메서드 호출 시 config 파라미터에 session_id를 전달하여 각 세션의 이력을 독립적으로 관리합니다. 이를 통해 여러 사용자나 여러 대화 스레드가 서로 영향을 주지 않고 병렬로 진행될 수 있습니다.

컨텍스트 주입 메커니즘은 자동화되어 있습니다. generate_response 메서드가 호출되면, RunnableWithMessageHistory는 현재 세션의 이력을 조회하여 프롬프트에 자동으로 포함시킵니다. 개발자는 이를 의식할 필요 없이 질문만 전달하면 됩니다. LLM은 전체 대화 이력을 컨텍스트로 받아 이전 대화를 참조한 답변을 생성할 수 있습니다.

컨텍스트 윈도우 관리는 ConversationSummaryMemory와 결합되어 작동합니다. 대화가 길어져 토큰 한도에 근접하면 자동으로 요약이 생성되고, 요약된 이력과 최근 메시지만 컨텍스트에 포함됩니다. 이를 통해 무제한으로 대화를 이어갈 수 있으면서도 토큰 사용량을 제어할 수 있습니다.

명시적 참조 해결은 향후 개선 과제입니다. 현재는 LLM이 암묵적으로 이전 대화를 참조하여 답변하지만, "그것", "저것", "앞서 말한"과 같은 지시어를 명시적으로 해결하는 메커니즘은 구현되지 않았습니다. 이는 LLM의 자연어 이해 능력에 의존하며, 대부분의 경우 충분히 작동하지만, 복잡한 참조 관계에서는 개선의 여지가 있습니다.

### 4.5. RAG 평가 시스템

#### 4.5.1. LLM Judge 기반 평가

RAG 시스템의 품질을 객관적으로 평가하는 것은 개선의 첫걸음입니다. 전통적인 평가 방법은 BLEU나 ROUGE와 같은 텍스트 유사도 지표를 사용하지만, 이는 의미적 정확성을 충분히 포착하지 못합니다. LLM Judge는 강력한 언어 모델을 평가자로 사용하여 더 정교한 평가를 수행합니다.

rag_evaluator 모듈의 evaluate_rag_performance 함수는 LLM Judge 기반 평가를 구현합니다. OpenAI 클라이언트를 초기화하고, 질의, 문서 텍스트, 질의 결과를 입력으로 받습니다. 평가에는 gpt-4o-mini 모델이 기본으로 사용되며, 비용 효율성과 평가 품질의 균형을 제공합니다.

시스템 프롬프트는 평가자의 역할을 명확히 정의합니다. "당신은 RAG 시스템의 품질을 평가하는 최고 전문가 LLM Judge입니다"라는 문구로 평가자의 전문성을 강조하고, 공정하고 엄격한 평가를 유도합니다. 이어서 평가 대상인 문서 전체, 질의, 질의 답변을 제시하고, 4가지 핵심 지표에 대해 1점부터 5점까지 점수를 매기도록 요청합니다.

평가 데이터 구조는 명확하게 라벨링되어 있습니다. 사용자 프롬프트에서 질의, 문서 전체, 질의 답변을 각각 구분하여 제시하므로 LLM이 각 요소를 정확히 파악할 수 있습니다. format 메서드를 사용하여 실제 값을 프롬프트에 삽입하며, 여러 줄에 걸친 텍스트도 올바르게 처리됩니다.

JSON 형식 응답을 강제하는 것이 중요합니다. 평가 결과를 파싱하여 프로그래매틱하게 처리하려면 구조화된 형식이 필요합니다. 프롬프트에 명시적으로 JSON 스키마를 제시하고, "결과는 반드시 다음의 JSON 형식으로만 출력해야 합니다"라는 지시를 포함하여 LLM이 텍스트 설명 없이 순수한 JSON만 반환하도록 유도합니다.

API 호출 및 오류 처리는 견고하게 구현되어 있습니다. try-except 블록으로 API 호출을 감싸고, 네트워크 오류나 API 한도 초과 등의 예외를 포착합니다. 응답이 성공적으로 받아지면 content를 추출하고 json.loads로 파싱합니다. JSON 파싱 실패 시에는 JSONDecodeError를 포착하여 원본 content와 함께 오류 메시지를 반환합니다. 이를 통해 평가 시스템이 예상치 못한 응답에도 중단되지 않고 디버깅 정보를 제공할 수 있습니다.

#### 4.5.2. 4대 평가 지표 (Faithfulness, Context Relevance, Answer Accuracy, Answer Relevance)

RAG 시스템의 품질은 다차원적으로 평가되어야 합니다. 본 시스템에서는 4가지 핵심 지표를 사용하여 종합적인 평가를 수행합니다.

Faithfulness(충실성/근거성)은 생성된 답변이 제공된 컨텍스트에 얼마나 충실한가를 평가합니다. LLM이 문서에 없는 내용을 추가하거나 왜곡하지 않고 사실에 기반하여 답변했는지 확인합니다. 5점은 모든 정보가 문서에서 직접 확인 가능하고 왜곡이 전혀 없는 경우입니다. 3점은 대부분 문서에 기반하지만 일부 추론이나 일반화가 포함된 경우입니다. 1점은 문서와 무관한 정보가 다수 포함되거나 사실이 왜곡된 경우입니다. Faithfulness는 환각 현상을 감지하는 핵심 지표로, RAG 시스템의 신뢰성을 직접적으로 나타냅니다.

Context Relevance(문맥 관련성)은 검색된 문서가 질의와 얼마나 관련 있는가를 평가합니다. 검색 단계의 품질을 측정하는 지표로, 관련 없는 문서가 많이 포함되면 낮은 점수를 받습니다. 5점은 모든 문서가 질의와 직접 관련되고 답변에 필요한 정보를 포함하는 경우입니다. 3점은 일부 관련 문서가 포함되었으나 무관한 내용도 섞여 있는 경우입니다. 1점은 대부분의 문서가 질의와 무관한 경우입니다. Context Relevance가 낮으면 검색 전략이나 임베딩 모델을 개선해야 함을 시사합니다.

Answer Accuracy(답변 정확도)는 답변이 질의에 대해 정확하고 완전한가를 평가합니다. 단순히 문서를 충실히 반영하는 것을 넘어 실제로 질문에 올바르게 답했는지 확인합니다. 5점은 질의에 정확하고 완전하게 답변하며 핵심 정보를 모두 포함하는 경우입니다. 3점은 부분적으로 정확하지만 일부 정보가 누락되거나 불명확한 경우입니다. 1점은 질의에 대한 답이 아니거나 완전히 잘못된 경우입니다. Answer Accuracy는 전체 시스템의 효용성을 나타내는 최종 지표입니다.

Answer Relevance(답변 관련성)은 답변이 질의와 직접적으로 관련되는가를 평가합니다. 정확하더라도 질문과 동떨어진 정보를 제공하면 낮은 점수를 받습니다. 5점은 답변이 질의에 직접 대응하며 불필요한 정보가 없는 경우입니다. 3점은 관련 있지만 일부 벗어난 내용이 포함된 경우입니다. 1점은 질의와 전혀 무관한 답변인 경우입니다. Answer Relevance는 프롬프트 설계의 효과를 측정하는 지표로, 낮을 경우 프롬프트를 더 구체적으로 개선해야 합니다.

각 지표에 대한 Reasoning 필드는 점수의 근거를 제공합니다. 단순히 점수만 제시하는 것이 아니라 왜 그 점수를 주었는지 설명함으로써, 평가 결과를 이해하고 개선 방향을 파악할 수 있습니다. LLM Judge는 상세한 이유를 작성하도록 프롬프트에 지시되어 있으며, 이는 평가의 투명성과 신뢰성을 높입니다.

#### 4.5.3. JSON 형식 결과 반환

평가 결과를 구조화된 형식으로 반환하는 것은 자동화된 분석과 모니터링을 가능하게 합니다. JSON 형식은 프로그래밍 언어에서 쉽게 파싱하고 처리할 수 있어 이상적입니다.

응답 스키마는 명확하게 정의되어 있습니다. 최상위 레벨에는 Query, Generated_Answer, Evaluation_Metrics, Overall_Assessment 네 개의 키가 있습니다. Query는 원래 질의를, Generated_Answer는 시스템이 생성한 답변을 그대로 포함합니다. Evaluation_Metrics는 4개의 지표를 포함하는 중첩된 객체로, 각 지표는 Rating과 Reasoning 필드를 가집니다. Overall_Assessment는 모든 지표를 종합한 전반적 평가를 문장으로 제공합니다.

파싱 과정은 두 단계로 이루어집니다. 먼저 OpenAI API 응답에서 message.content를 추출합니다. 이 content는 JSON 문자열이어야 하지만, LLM이 markdown 코드 블록으로 감싸는 경우가 있습니다. 예를 들어 json으로 시작하고 로 끝나는 형태입니다. 이를 처리하기 위해 정규표현식으로 백틱을 제거한 후 json.loads를 호출합니다.

JSON 파싱 오류는 적절히 처리됩니다. LLM이 잘못된 형식을 반환하면 JSONDecodeError가 발생하는데, 이를 포착하여 오류 딕셔너리를 반환합니다. 오류 딕셔너리에는 error 키에 오류 메시지가, raw_content 키에 원본 응답이 포함되어 디버깅을 용이하게 합니다. 이를 통해 평가 시스템이 중단되지 않고 문제를 보고할 수 있습니다.

반환된 딕셔너리는 다양한 방식으로 활용됩니다. 직접 출력하여 사람이 읽을 수 있는 형태로 평가 결과를 확인할 수 있습니다. 데이터베이스에 저장하여 시간에 따른 품질 추이를 추적할 수 있습니다. 통계 분석을 통해 평균 점수를 계산하거나 지표별 분포를 시각화할 수 있습니다. 자동화된 테스트에 통합하여 품질 저하를 조기에 감지하는 CI/CD 파이프라인을 구축할 수 있습니다.

전체 평가 워크플로우는 다음과 같이 진행됩니다. 테스트 질의 세트를 준비하고 각 질의에 대해 RAG 시스템을 실행합니다. 생성된 답변과 검색된 문서를 evaluate_rag_performance 함수에 전달합니다. 반환된 JSON 결과를 파싱하여 각 지표의 점수를 추출합니다. 여러 질의에 대한 결과를 집계하여 시스템의 전반적 성능을 평가합니다. 낮은 점수를 받은 지표를 중심으로 개선 작업을 수행합니다. 개선 후 재평가하여 효과를 측정합니다.

---
