---
layout: default
title: "[중급프로젝트] RAG 기반 정부나라장터 입찰공고 분석 시스템 - 임베딩 처리 및 벡터 저장"
description: "[중급프로젝트] RAG 기반 정부나라장터 입찰공고 분석 시스템 - 임베딩 처리 및 벡터 저장"
date: 2025-11-25
cache-control: no-cache
expires: 0
pragma: no-cache
author: "김명환 (머신러닝 엔지니어)"
---

## 3. 임베딩 처리 및 벡터 저장 (김명환)

### 3.1. 임베딩 전처리 전략

#### 3.1.1. 3단계 전처리 파이프라인

RAG 시스템의 성능을 결정짓는 핵심 요소 중 하나는 문서 전처리의 품질입니다. 본 시스템에서는 원본 보존과 처리 최적화라는 두 가지 목표를 균형있게 달성하기 위해 3단계 전처리 파이프라인을 설계하였습니다.

첫 번째 단계는 DocumentProcessor의 clean_markdown_text 메서드에서 수행되는 최소 전처리입니다. 이 단계는 PDF에서 Markdown으로 변환된 직후에 실행되며, 원본 텍스트의 내용과 구조를 최대한 보존하면서 형식적인 문제만을 해결합니다. 공백과 탭을 단일 공백으로 변환하고, 연속된 세 줄 이상의 개행을 두 줄로 축소하며, 각 라인의 앞뒤 공백을 제거합니다. 이 단계의 결과는 DocumentsDB의 text_content 컬럼에 저장되어 원본 데이터로서 보존됩니다.

두 번째 단계는 EmbeddingProcessor의 clean_markdown_text 메서드에서 수행되는 최대 전처리입니다. 이 단계는 청킹 전에 실행되며, 임베딩 품질을 최적화하기 위해 공격적인 정제 작업을 수행합니다. 코드 블록, 수식, 페이지 마커 등 보호해야 할 요소는 임시 플레이스홀더로 마스킹하여 보존하고, Markdown 요소인 HTML 태그, 이미지, 링크, 강조 기호, 헤더, 인용구 등을 제거합니다. 공백과 개행을 정리하고, 빈 테이블 행과 과도한 목차 구분선을 축약한 후, 보호된 요소를 다시 복원합니다.

세 번째 단계는 EmbeddingProcessor의 clean_page_text 메서드에서 수행되는 페이지별 정제입니다. 이 단계는 페이지 단위로 분할된 후 각 페이지에 대해 실행되며, 페이지 마커를 제거하고 최종 공백 정리를 수행합니다. ERROR_PAGE_MARKER와 EMPTY_PAGE_MARKER, 그리고 페이지 번호 마커를 모두 제거하여 순수한 텍스트만 남깁니다.

이러한 3단계 파이프라인 구조는 여러 장점을 제공합니다. 원본 데이터는 최소 전처리만 거친 상태로 보존되어 향후 다른 전처리 방식을 실험할 때 처음부터 다시 시작할 수 있습니다. 최대 전처리는 임베딩 직전에 수행되므로 설정을 변경하더라도 PDF 변환을 다시 할 필요가 없습니다. 페이지별 정제는 청킹 이후에 수행되므로 페이지 마커를 활용한 분할이 가능합니다.

#### 3.1.2. 보호 블록 마스킹 기법

Markdown 전처리 과정에서 모든 요소를 무차별적으로 제거하면 중요한 정보가 손실될 수 있습니다. 특히 기술 제안서에는 코드 예제, 수학 공식, 다이어그램 등이 포함되는 경우가 많으며, 이러한 요소는 문서의 핵심 내용을 담고 있습니다. 보호 블록 마스킹 기법은 이러한 요소들을 전처리 과정에서 보호하기 위해 설계되었습니다.

마스킹 프로세스는 정규표현식을 사용하여 보호할 블록을 식별하고, 각 블록을 고유한 플레이스홀더로 치환하는 방식으로 진행됩니다. Config 설정의 MARKDOWN_PROTECT_BLOCKS 리스트에 정의된 블록 타입만 보호되며, 기본값으로는 code, math, inline_math, mermaid가 포함됩니다.

코드 블록 보호는 4개의 백틱으로 둘러싸인 블록을 우선 처리한 후 3개의 백틱으로 둘러싸인 블록을 처리합니다. 이는 중첩된 코드 블록을 올바르게 처리하기 위한 전략입니다. 각 코드 블록은 XPROTECTEDXCODE4XnX 또는 XPROTECTEDXCODE3XnX 형식의 플레이스홀더로 치환되며, n은 블록의 순서를 나타내는 카운터입니다. 원본 코드 블록 내용은 protected_blocks 딕셔너리에 플레이스홀더를 키로 하여 저장됩니다.

수식 블록 보호는 달러 기호 두 개로 둘러싸인 블록 수식과 달러 기호 하나로 둘러싸인 인라인 수식을 각각 처리합니다. 블록 수식은 XPROTECTEDXMATHXnX 형식으로, 인라인 수식은 XPROTECTEDXINLINEXnX 형식으로 치환됩니다. 수식은 LaTeX 문법을 사용하므로 많은 특수 문자를 포함하는데, 마스킹을 통해 이러한 문자들이 후속 전처리 단계에서 의도치 않게 변경되는 것을 방지합니다.

페이지 마커는 항상 보호됩니다. ERROR_PAGE_MARKER, EMPTY_PAGE_MARKER, 그리고 페이지 번호 마커는 각각 XPROTECTEDXMARKERXnX 형식의 플레이스홀더로 치환되어 protected_markers 딕셔너리에 저장됩니다. 이를 통해 페이지 마커가 전처리 과정에서 손상되지 않고 페이지 분할 단계까지 안전하게 전달됩니다.

마스킹 이후 일반적인 전처리 작업이 수행됩니다. 탈출문자 처리, HTML 태그 제거, 이미지 제거, 링크 제거, 강조 기호 제거, 헤더 제거, 인용구 제거, 공백 정리 등의 작업이 플레이스홀더를 포함하는 텍스트에 대해 수행됩니다. 플레이스홀더는 영문자와 숫자, 언더스코어로만 구성되어 있어 대부분의 전처리 작업에 영향을 받지 않습니다.

전처리가 완료되면 복원 단계가 진행됩니다. protected_blocks와 protected_markers 딕셔너리를 역순으로 순회하면서 플레이스홀더를 원본 내용으로 치환합니다. 역순 처리는 중첩된 블록이나 인덱스 충돌을 방지하기 위한 안전 장치입니다.

이러한 마스킹 기법은 선택적 보호를 가능하게 합니다. Config 설정을 통해 어떤 블록을 보호할지 제어할 수 있으며, 특정 문서 유형에 따라 전략을 조정할 수 있습니다. 예를 들어 기술 문서가 아닌 행정 문서의 경우 코드 블록 보호를 비활성화하여 처리 속도를 높일 수 있습니다.

#### 3.1.3. Markdown 요소 제거 및 정제

보호 블록 마스킹이 완료된 후에는 일반적인 Markdown 요소들을 제거하는 작업이 수행됩니다. 이 단계의 목표는 문서의 의미를 담고 있는 텍스트만 남기고, 형식을 위한 마크업은 제거하는 것입니다.

탈출문자 처리는 백슬래시로 이스케이프된 특수 문자를 원래 문자로 복원합니다. Markdown에서는 별표, 언더스코어, 대괄호 등의 특수 문자를 리터럴 문자로 표시하기 위해 백슬래시를 앞에 붙이는데, 정규표현식 패턴 r'\\([*_\[\]()#+-])'를 사용하여 이러한 이스케이프 시퀀스를 찾아 백슬래시를 제거합니다.

HTML 태그 제거는 Config의 MARKDOWN_REMOVE_ELEMENTS에 html이 포함된 경우 수행됩니다. 정규표현식 패턴 r'<[^>]+>'를 사용하여 여는 태그와 닫는 태그를 모두 찾아 공백으로 치환합니다. 예를 들어 "사업\<div\>개요\</div\>"는 "사업 개요"로 변환됩니다. 일부 공고 문서는 HTML 편집기로 작성된 후 PDF로 변환되어 HTML 태그가 포함되는 경우가 있는데, 이를 제거함으로써 깔끔한 텍스트를 얻을 수 있습니다.

이미지 제거는 Markdown 이미지 문법인 !\[대체텍스트\](URL)을 찾아 삭제합니다. 정규표현식 패턴 r'!\[([^\]]*)\]\([^\)]+\)'를 사용하며, 이미지는 텍스트 정보를 포함하지 않으므로 완전히 제거됩니다. 향후 멀티모달 RAG 시스템으로 확장할 경우 이 단계를 조정하여 이미지를 별도로 처리할 수 있습니다.

링크 제거는 Markdown 링크 문법인 \[텍스트\](URL)에서 URL 부분만 제거하고 텍스트는 유지합니다. 정규표현식 패턴 r'\[([^\]]+)\]\([^\)]+\)'를 사용하여 링크를 찾고, 첫 번째 캡처 그룹인 텍스트만 남깁니다. 예를 들어 "\[나라장터\](https://g2b.go.kr)"는 "나라장터"로 변환됩니다.

강조 기호 제거는 볼드체를 나타내는 별표 두 개와 이탤릭체를 나타내는 별표 하나를 제거합니다. 정규표현식 패턴 r'\*\*([^\*]+)\*\*'와 r'\*([^\*]+)\*'를 순차적으로 적용하여 강조된 텍스트에서 별표만 제거하고 내용은 유지합니다. 취소선을 나타내는 물결표 두 개도 유사한 방식으로 제거됩니다.

헤더 제거는 Markdown 헤더 문법인 샵 기호를 제거합니다. 정규표현식 패턴 r'^#{1,6}\s+'를 사용하여 라인 시작 부분의 샵 기호와 뒤따르는 공백을 제거합니다. 예를 들어 "## 사업 개요"는 "사업 개요"로 변환됩니다. 헤더는 문서의 구조를 나타내지만, 임베딩에서는 텍스트 내용만 중요하므로 제거합니다.

인용구 제거는 각 라인 시작 부분의 꺾쇠 기호를 제거합니다. 정규표현식 패턴 r'^\>\s+'를 사용하며, 인용된 텍스트는 유지하되 인용 기호만 제거합니다.

리스트 마커 제거는 순서 있는 리스트의 숫자와 점, 순서 없는 리스트의 별표, 대시, 플러스 기호를 제거합니다. 정규표현식 패턴 r'^\s*[\*\-\+]\s+'와 r'^\s*\d+\.\s+'를 사용하여 라인 시작 부분의 리스트 마커를 제거합니다.

공백 정리는 여러 단계로 이루어집니다. 라인 시작 부분의 공백을 제거하고, 연속된 세 줄 이상의 개행을 두 줄로 축소하며, 연속된 공백과 탭을 단일 공백으로 변환합니다. 또한 빈 테이블 행인 파이프 기호와 공백만으로 이루어진 라인을 제거하고, 목차에서 자주 사용되는 긴 점선을 세 개의 점으로 축약합니다.

이러한 정제 작업의 결과로 문서는 순수한 텍스트 형태가 됩니다. Markdown 문법이나 HTML 태그 없이 문서의 실제 내용만 남게 되어, 임베딩 모델이 의미에 집중할 수 있게 됩니다.

### 3.2. 청킹 전략 (Chunking Strategy)

#### 3.2.1. RecursiveCharacterTextSplitter 설정

문서를 적절한 크기의 청크로 분할하는 것은 RAG 시스템의 핵심 과제입니다. 청크가 너무 크면 검색 정밀도가 떨어지고, 너무 작으면 문맥이 손실됩니다. 본 시스템에서는 LangChain의 RecursiveCharacterTextSplitter를 사용하여 균형잡힌 청킹을 구현하였습니다.

RecursiveCharacterTextSplitter는 텍스트를 재귀적으로 분할하는 방식으로 작동합니다. 먼저 큰 구분자로 텍스트를 나누고, 결과 청크가 여전히 목표 크기보다 크면 더 작은 구분자로 다시 나눕니다. 이 과정은 청크가 목표 크기 이하가 될 때까지 반복됩니다.

구분자의 우선순위는 Config의 CHUNK_SEPARATORS 리스트에 정의되어 있으며, 기본값은 이중 개행, 단일 개행, 공백, 빈 문자열 순서입니다. 이는 먼저 문단 단위로 분할을 시도하고, 그것이 불가능하면 문장 단위로, 그것도 불가능하면 단어 단위로, 최후에는 문자 단위로 분할한다는 의미입니다.

chunk_size 파라미터는 각 청크의 목표 크기를 지정합니다. Config의 CHUNK_SIZE 설정값을 사용하며, 기본값은 1500입니다. CHUNKING_MODE가 token으로 설정된 경우 이는 토큰 수를 의미하고, character로 설정된 경우 문자 수를 의미합니다. 본 시스템에서는 토큰 기반 청킹을 사용하여 OpenAI API의 토큰 한도를 효율적으로 관리합니다.

chunk_overlap 파라미터는 인접한 청크 간에 중복되는 텍스트의 양을 지정합니다. Config의 CHUNK_OVERLAP 설정값을 사용하며, 기본값은 300입니다. 중복은 청크 경계에서 문맥이 단절되는 것을 방지하는 역할을 합니다. 예를 들어 한 문장이 두 청크에 걸쳐 있을 때 중복 영역 덕분에 양쪽 청크 모두에서 완전한 문장을 포함할 수 있습니다.

length_function 파라미터는 텍스트 길이를 측정하는 함수를 지정합니다. 토큰 기반 청킹을 위해 tiktoken 라이브러리를 사용하는 커스텀 함수가 제공됩니다. 이 함수는 OpenAI의 토크나이저를 사용하여 텍스트를 정확히 토큰 수로 변환하므로, API 호출 시 예상 토큰 수와 일치합니다.

is_separator_regex 파라미터는 구분자를 정규표현식으로 해석할지 여부를 지정합니다. 기본값은 False이며, 구분자는 리터럴 문자열로 처리됩니다. 개행 문자인 \n은 이스케이프 시퀀스가 아닌 실제 개행 문자로 해석됩니다.

RecursiveCharacterTextSplitter의 초기화는 EmbeddingProcessor의 생성자에서 수행됩니다. Config 설정을 기반으로 splitter 인스턴스가 생성되며, 이는 모든 문서 처리에 재사용됩니다. 이를 통해 일관된 청킹 전략이 적용되고 설정 변경 시 한 곳만 수정하면 됩니다.

#### 3.2.2. 페이지 단위 분할 방식

청킹 과정에서 페이지 경계를 고려하는 것은 출처 추적과 사용자 경험 측면에서 매우 중요합니다. 본 시스템에서는 페이지 단위 분할 방식을 통해 각 청크가 어느 페이지 범위에 속하는지 정확히 기록합니다.

페이지 단위 분할은 DocumentProcessor가 삽입한 페이지 마커를 기준으로 이루어집니다. EmbeddingProcessor의 process_document 메서드는 먼저 전체 텍스트를 페이지 마커로 분리하여 페이지 단위 텍스트 리스트를 생성합니다. 정규표현식 패턴 r'---\s*페이지\s+(\d+)\s*---'를 사용하여 페이지 번호를 추출하고, split 함수로 텍스트를 분할합니다.

각 페이지는 순차적으로 처리됩니다. 페이지 텍스트가 ERROR_PAGE_MARKER나 EMPTY_PAGE_MARKER를 포함하는 경우 해당 페이지는 건너뜁니다. 정상 페이지의 경우 clean_page_text 메서드로 마커를 제거하고 최종 정제를 수행한 후, 버퍼에 추가합니다.

버퍼는 여러 페이지의 텍스트를 누적하는 역할을 합니다. 단일 페이지가 CHUNK_SIZE보다 작은 경우가 많기 때문에, 인접한 페이지들을 병합하여 적절한 크기의 청크를 만들 필요가 있습니다. 버퍼에는 텍스트뿐만 아니라 시작 페이지 번호와 종료 페이지 번호도 함께 기록됩니다.

버퍼 크기가 CHUNK_SIZE 이상이 되거나 마지막 페이지에 도달하면 청킹 작업이 트리거됩니다. 버퍼의 토큰 수가 CHUNK_SIZE 이하인 경우 버퍼 전체를 단일 청크로 추가하고, chunk_type을 single 또는 merged로 설정합니다. single은 단일 페이지로 구성된 청크를, merged는 여러 페이지가 병합된 청크를 나타냅니다.

버퍼의 토큰 수가 CHUNK_SIZE를 초과하는 경우 RecursiveCharacterTextSplitter를 사용하여 버퍼를 여러 청크로 분할합니다. split_text 메서드가 호출되어 적절한 구분자를 기준으로 재귀적 분할이 수행되고, 결과 청크들의 chunk_type은 split으로 설정됩니다.

페이지 범위 정보는 각 청크의 메타데이터에 start_page와 end_page 필드로 저장됩니다. 병합된 청크의 경우 여러 페이지에 걸쳐 있으므로 범위가 넓어지고, 분할된 청크의 경우 모두 동일한 페이지 범위를 가질 수 있습니다. 이 정보는 검색 결과를 사용자에게 제시할 때 출처를 명시하는 데 사용됩니다.

청킹이 완료된 후 버퍼는 초기화되고, 다음 페이지 그룹의 처리를 위해 준비됩니다. 이러한 버퍼 기반 접근 방식은 메모리 효율성과 청킹 품질을 모두 달성합니다. 전체 문서를 메모리에 로드하지 않고 페이지 단위로 스트리밍 처리할 수 있으며, 페이지 경계를 넘는 자연스러운 청크를 생성할 수 있습니다.

#### 3.2.3. 메타데이터 보존 전략

각 청크에는 풍부한 메타데이터가 첨부되어 검색과 추적을 용이하게 합니다. 메타데이터는 청크의 출처, 내용, 생성 조건 등을 포괄적으로 기록합니다.

file_hash는 원본 파일의 SHA-256 해시값으로 청크가 어느 문서에서 나왔는지 식별합니다. 이를 통해 특정 파일의 모든 청크를 검색하거나 파일 단위로 벡터를 삭제할 수 있습니다.

file_name은 사용자에게 친숙한 파일 이름으로 검색 결과를 표시할 때 사용됩니다. file_hash만으로는 사용자가 어느 문서인지 직관적으로 알기 어렵기 때문에 함께 저장됩니다.

start_page와 end_page는 청크가 속한 페이지 범위를 나타냅니다. 단일 페이지 청크의 경우 두 값이 동일하고, 병합 청크의 경우 시작 페이지와 종료 페이지가 다릅니다. 사용자는 이 정보를 통해 원본 문서의 정확한 위치로 바로 이동할 수 있습니다.

chunk_type은 청크가 어떻게 생성되었는지 나타냅니다. single은 단일 페이지가 그대로 청크가 된 경우, merged는 여러 페이지가 병합된 경우, split은 큰 텍스트가 분할된 경우입니다. 이 정보는 청킹 전략을 분석하고 개선하는 데 유용합니다.

chunk_index는 동일 파일 내에서 청크의 순서를 나타내는 0부터 시작하는 인덱스입니다. 이를 통해 청크들을 원래 순서대로 정렬하거나, 인접한 청크를 찾을 수 있습니다.

embedding_config_hash는 파일 해시와 임베딩 설정을 결합한 통합 해시값입니다. 이는 재현성 보장의 핵심 메커니즘으로, 파일 내용이나 청킹 설정이 변경되면 이 해시값도 달라집니다. VectorStoreManager는 이 값을 비교하여 재임베딩이 필요한지 자동으로 판단합니다.

chunk_hash는 청크 텍스트 내용의 SHA-256 해시값입니다. 동일한 위치의 청크라도 내용이 변경되면 이 해시값이 달라지므로, 증분 업데이트 시 실제로 변경된 청크만 재처리할 수 있습니다.

재현성을 위한 설정 정보도 메타데이터에 포함됩니다. config_chunk_size, config_chunk_overlap, config_chunking_mode, config_chunk_separators, config_markdown_max_lines 등은 청크가 생성될 당시의 설정값을 기록합니다. 이를 통해 나중에 동일한 설정으로 청크를 재생성할 수 있으며, 설정 변경의 영향을 분석할 수 있습니다.

embedding_version은 사용된 임베딩 모델의 이름을 기록합니다. OpenAI는 주기적으로 임베딩 모델을 업데이트하는데, 모델이 변경되면 동일한 텍스트라도 다른 벡터가 생성됩니다. 이 필드를 통해 어떤 모델로 생성된 벡터인지 추적할 수 있습니다.

created_at은 청크가 생성된 시각을 ISO 8601 형식의 문자열로 기록합니다. 타임존은 KST로 설정되어 한국 시간을 반영합니다. 이 정보는 시간 기반 필터링이나 데이터 갱신 주기 분석에 활용됩니다.

메타데이터는 JSON 직렬화가 가능한 형태로 저장되어 LangChain의 Document 객체에 포함됩니다. VectorStoreManager는 이 메타데이터를 FAISS 인덱스와 함께 저장하고, 검색 시 결과와 함께 반환합니다.

### 3.3. 벡터 임베딩

#### 3.3.1. OpenAI text-embedding-3-small 모델

벡터 임베딩은 텍스트를 고차원 벡터 공간의 점으로 변환하는 과정입니다. 의미가 유사한 텍스트는 벡터 공간에서 가까운 위치에 놓이게 되어 코사인 유사도나 유클리드 거리를 통해 검색할 수 있습니다.

본 시스템에서는 OpenAI의 text-embedding-3-small 모델을 사용합니다. 이 모델은 2024년 초에 공개된 최신 임베딩 모델로, 이전 세대인 text-embedding-ada-002 대비 성능이 크게 향상되었습니다. 1536차원의 벡터를 생성하며, 다양한 언어와 도메인에서 우수한 성능을 보입니다.

text-embedding-3-small의 장점은 여러 가지입니다. 첫째는 비용 효율성입니다. 백만 토큰당 0.02달러로 이전 모델 대비 5배 저렴하여 대규모 문서 처리에 적합합니다. 둘째는 처리 속도입니다. API 응답 시간이 빠르고 배치 처리를 지원하여 수천 개의 청크를 신속히 임베딩할 수 있습니다. 셋째는 다국어 지원입니다. 한국어를 포함한 100개 이상의 언어를 지원하며, 영어 중심 모델에 비해 한국어 텍스트의 의미를 더 정확히 포착합니다. 넷째는 긴 컨텍스트 처리 능력입니다. 최대 8191 토큰까지 처리할 수 있어 긴 청크도 문제없이 임베딩할 수 있습니다.

임베딩 생성 프로세스는 LangChain의 OpenAIEmbeddings 클래스를 통해 추상화됩니다. 이 클래스는 OpenAI API 호출을 관리하고, 오류 처리와 재시도 로직을 내장하고 있습니다. EmbeddingProcessor는 이 클래스의 인스턴스를 생성하여 VectorStoreManager에 전달합니다.

배치 처리는 API 호출 횟수를 줄이고 처리 속도를 높이는 핵심 최적화 기법입니다. Config의 EMBEDDING_BATCH_SIZE 설정에 따라 여러 청크를 한 번의 API 호출로 처리합니다. 기본값은 100으로 설정되어 있어, 100개의 청크가 모일 때마다 배치로 전송됩니다. OpenAI API는 단일 요청에서 여러 텍스트를 동시에 임베딩할 수 있어 효율적입니다.

오류 처리는 네트워크 문제나 API 한도 초과 등의 상황에 대비합니다. LangChain의 OpenAIEmbeddings는 자동 재시도 메커니즘을 포함하고 있으며, 지수 백오프 전략을 사용하여 일시적 오류를 극복합니다. 재시도 횟수는 3회로 제한되며, 모든 재시도가 실패하면 예외가 발생하여 상위 레벨에서 처리됩니다.

#### 3.3.2. 임베딩 설정 해시 (embedding_config_hash)

임베딩 벡터의 재현성을 보장하고 설정 변경을 추적하기 위해 임베딩 설정 해시 메커니즘이 도입되었습니다. embedding_config_hash는 파일 해시와 임베딩 관련 모든 설정을 결합하여 계산한 SHA-256 해시값입니다.

해시 계산에 포함되는 요소는 다음과 같습니다. file_hash는 원본 파일의 내용을 나타내며, 파일이 변경되면 해시도 달라집니다. chunk_size와 chunk_overlap은 청킹 전략의 핵심 파라미터로, 이 값이 변경되면 청크의 크기와 중복 정도가 달라집니다. chunking_mode는 토큰 기반인지 문자 기반인지를 나타내며, 이 역시 청킹 결과에 영향을 줍니다. embedding_model은 사용된 임베딩 모델의 이름으로, 모델이 변경되면 완전히 다른 벡터가 생성됩니다. chunk_separators는 청킹 시 사용된 구분자의 우선순위로, 이것이 달라지면 청크 경계가 변경됩니다. markdown_max_lines는 코드나 수식 블록의 최대 라인 수 제한으로, 전처리 결과에 영향을 줍니다. protect_blocks와 remove_elements는 어떤 요소를 보호하고 제거할지 결정하는 설정으로, 전처리된 텍스트 내용에 직접 영향을 미칩니다.

이러한 요소들은 딕셔너리로 구성되고 JSON 문자열로 직렬화된 후, SHA-256 해시 함수에 입력됩니다. 딕셔너리 키는 알파벳 순으로 정렬되어 일관된 해시값을 보장합니다. 결과 해시값은 64자리 16진수 문자열로 표현됩니다.

embedding_config_hash의 활용은 다양합니다. 첫째는 중복 방지입니다. VectorStoreManager의 chunk_map은 file_hash와 chunk_index의 조합을 키로 사용하는데, 값에 embedding_config_hash를 포함합니다. 동일한 파일과 청크 인덱스에 대해 새로운 벡터를 추가하려 할 때, 기존 embedding_config_hash와 비교하여 설정이 동일하면 스킵하고, 다르면 기존 벡터를 삭제하고 새 벡터를 추가합니다.

둘째는 자동 재임베딩입니다. EmbeddingProcessor의 sync_with_docs_db 메서드는 DocumentsDB의 모든 파일과 VectorStoreManager의 벡터를 비교합니다. 양쪽 모두 존재하는 파일에 대해 embedding_config_hash를 계산하고, 저장된 값과 비교하여 다르면 재임베딩 대상으로 분류합니다. 이를 통해 설정 변경 후 자동으로 영향받는 파일만 재처리할 수 있습니다.

셋째는 실험 추적입니다. 다양한 청킹 전략이나 전처리 방식을 실험할 때, 각 설정 조합은 고유한 embedding_config_hash를 가집니다. 실험 결과와 해시값을 함께 기록하면, 어떤 설정이 최적의 성능을 보였는지 명확히 파악할 수 있습니다.

#### 3.3.3. 재현성 보장 메커니즘

RAG 시스템의 성능을 개선하고 문제를 디버깅하기 위해서는 동일한 입력에 대해 동일한 결과를 재현할 수 있어야 합니다. 본 시스템에서는 여러 계층에서 재현성을 보장하는 메커니즘을 구현하였습니다.

파일 수준 재현성은 file_hash를 통해 보장됩니다. 동일한 PDF 파일은 언제 어디서 처리하더라도 같은 해시값을 생성하며, DocumentsDB에 저장된 text_content는 항상 동일합니다. 이는 임베딩 처리의 시작점이 일관됨을 보장합니다.

설정 수준 재현성은 embedding_config_hash를 통해 보장됩니다. 모든 청킹 및 전처리 설정이 해시에 포함되어, 설정이 동일하면 같은 청크가 생성됩니다. Config 클래스는 JSON 파일로 설정을 저장하고 로드하므로, 언제든지 과거의 설정을 복원할 수 있습니다.

청크 수준 재현성은 chunk_hash를 통해 보장됩니다. 청크의 텍스트 내용이 동일하면 같은 해시값을 가지며, 임베딩 모델에 동일한 입력이 전달됩니다. OpenAI의 임베딩 모델은 결정론적이므로 같은 텍스트는 항상 같은 벡터를 생성합니다.

메타데이터 수준 재현성은 청크에 첨부된 모든 설정 정보를 통해 보장됩니다. config_chunk_size, config_chunk_overlap 등의 필드는 청크가 생성될 당시의 설정을 완전히 기록합니다. 이를 통해 나중에 정확히 같은 조건으로 청크를 재생성할 수 있습니다.

버전 관리는 embedding_version 필드를 통해 이루어집니다. 임베딩 모델이 업데이트되면 이 필드를 확인하여 어떤 벡터가 재임베딩이 필요한지 판단할 수 있습니다. 예를 들어 text-embedding-3-small에서 차세대 모델로 업그레이드할 때, 모든 벡터를 한번에 재생성하는 대신 점진적으로 마이그레이션할 수 있습니다.

타임스탬프는 created_at 필드를 통해 기록됩니다. 언제 벡터가 생성되었는지 알 수 있어 시간 기반 필터링이나 데이터 라이프사이클 관리가 가능합니다. 예를 들어 30일 이상 된 벡터는 아카이브하고 최신 데이터만 활성 인덱스에 유지할 수 있습니다.

### 3.4. FAISS 벡터 저장소

#### 3.4.1. 인덱스 구조 및 관리

FAISS는 Facebook AI Research에서 개발한 고성능 벡터 유사도 검색 라이브러리입니다. 수백만 개의 벡터를 효율적으로 저장하고 검색할 수 있어 대규모 RAG 시스템에 적합합니다.

본 시스템에서는 FAISS의 IndexFlatL2 인덱스를 사용합니다. 이는 L2 거리 기반의 정확한 최근접 이웃 검색을 수행하는 인덱스로, 양자화나 근사 알고리즘을 사용하지 않아 검색 결과가 정확합니다. 인덱스는 1536차원의 부동소수점 벡터를 저장하며, 각 벡터는 6144바이트의 메모리를 차지합니다.

인덱스 생성은 LangChain의 FAISS.from_documents 메서드를 통해 수행됩니다. 초기 문서 리스트와 임베딩 객체를 전달하면, 내부적으로 텍스트를 임베딩하고 FAISS 인덱스를 생성한 후, LangChain의 FAISS 래퍼로 감쌉니다. 이 래퍼는 docstore와 index_to_docstore_id 매핑을 관리하여 벡터와 원본 문서를 연결합니다.

docstore는 Document 객체를 저장하는 딕셔너리입니다. 각 Document는 page_content와 metadata를 포함하며, 고유한 ID로 식별됩니다. index_to_docstore_id는 FAISS 인덱스의 벡터 인덱스를 docstore ID로 매핑하는 딕셔너리입니다. 예를 들어 FAISS 인덱스의 0번 벡터가 docstore의 abc123 ID를 가진 문서에 해당한다면, index_to_docstore_id[0] = 'abc123'으로 저장됩니다.

인덱스 저장은 save_local 메서드를 통해 수행됩니다. FAISS 인덱스는 vectorstore.faiss 파일에 바이너리 형태로 저장되고, docstore와 index_to_docstore_id는 vectorstore.pkl 파일에 pickle 형식으로 저장됩니다. 이 두 파일이 함께 있어야 완전한 벡터스토어를 복원할 수 있습니다.

인덱스 로드는 load_local 메서드를 통해 수행됩니다. 파일이 존재하면 FAISS 인덱스와 메타데이터를 로드하여 FAISS 래퍼 객체를 재구성합니다. 파일이 없으면 VectorStoreManager는 자동으로 더미 인덱스를 생성하여 초기 상태를 만듭니다.

#### 3.4.2. 메타데이터 매핑 전략

FAISS 자체는 벡터만 저장하고 메타데이터를 지원하지 않습니다. LangChain의 FAISS 래퍼는 이를 보완하기 위해 docstore와 매핑 구조를 사용하지만, 고급 쿼리 기능은 제한적입니다. 본 시스템에서는 chunk_map이라는 추가 자료구조를 도입하여 효율적인 메타데이터 관리를 구현하였습니다.

chunk_map은 파일 해시와 청크 인덱스의 조합을 키로, FAISS 인덱스와 해시 정보를 값으로 하는 딕셔너리입니다. 구조는 Dict[Tuple[str, int], Tuple[int, str, str]] 형태로, 키는 file_hash와 chunk_index의 튜플이고, 값은 faiss_idx, chunk_hash, embedding_config_hash의 튜플입니다.

chunk_map의 구축은 _build_chunk_map 메서드에서 수행됩니다. docstore의 모든 Document를 순회하면서 메타데이터에서 file_hash, chunk_index, chunk_hash, embedding_config_hash를 추출하고, index_to_docstore_id를 통해 FAISS 인덱스를 찾아 chunk_map에 저장합니다. 이 과정은 인덱스 로드 시 자동으로 수행되어 메모리 상에 매핑 정보를 준비합니다.

chunk_map의 활용은 다양합니다. 첫째는 빠른 중복 검사입니다. 새로운 청크를 추가할 때 chunk_map에서 file_hash와 chunk_index 조합을 찾아 이미 존재하는지 즉시 확인할 수 있습니다. 딕셔너리 조회는 O(1) 시간복잡도이므로 매우 빠릅니다.

둘째는 내용 변경 감지입니다. 중복 청크가 발견되면 chunk_hash를 비교하여 내용이 동일한지 확인합니다. 내용이 다르면 기존 벡터를 삭제하고 새 벡터로 교체합니다. 이를 통해 문서가 업데이트되었을 때 변경된 부분만 재처리할 수 있습니다.

셋째는 설정 변경 감지입니다. embedding_config_hash를 비교하여 청킹이나 전처리 설정이 변경되었는지 확인합니다. 설정이 다르면 재임베딩이 필요하다고 판단하여 기존 벡터를 삭제하고 새로 생성합니다.

넷째는 효율적인 삭제입니다. 특정 파일의 모든 청크를 삭제할 때 chunk_map을 순회하여 해당 file_hash를 가진 항목들의 FAISS 인덱스를 수집하고, 한번에 삭제합니다. 전체 docstore를 순회할 필요 없이 빠르게 대상을 찾을 수 있습니다.

chunk_map은 메모리에만 존재하며 디스크에 저장되지 않습니다. 인덱스 로드 시마다 docstore에서 재구축되므로 일관성이 보장됩니다. 메모리 오버헤드는 크지 않은데, 각 항목이 튜플 두 개만 저장하고, 청크 개수가 수백만 개가 되지 않는 한 수십 메가바이트 이내로 유지됩니다.

#### 3.4.3. 검색 성능 최적화

FAISS는 그 자체로 고성능 라이브러리이지만, 적절한 사용 패턴과 설정을 통해 더욱 최적화할 수 있습니다.

인덱스 타입 선택은 성능과 정확도의 트레이드오프입니다. IndexFlatL2는 정확한 검색을 보장하지만, 벡터 수가 증가하면 검색 시간도 선형으로 증가합니다. 본 시스템의 프로토타입 단계에서는 수천에서 수만 개의 벡터를 다루므로 IndexFlatL2가 적합하지만, 향후 수십만 개 이상으로 확장될 경우 IndexIVFFlat이나 IndexHNSW와 같은 근사 검색 인덱스로 전환을 고려할 수 있습니다.

배치 검색은 여러 쿼리를 한번에 처리하여 오버헤드를 줄입니다. FAISS의 search 메서드는 쿼리 벡터를 2차원 배열로 받아 한번에 여러 검색을 수행할 수 있습니다. 본 시스템에서는 단일 쿼리를 주로 사용하지만, 향후 배치 질의 기능을 추가할 경우 이를 활용할 수 있습니다.

top_k 파라미터 조정은 검색 속도에 영향을 줍니다. 더 많은 결과를 요청할수록 검색 시간이 증가하므로, 필요 이상으로 큰 값을 설정하지 않아야 합니다. Config의 TOP_K_SUMMARY 기본값은 5로 설정되어 있어, 대부분의 경우 충분하면서도 빠른 검색을 보장합니다.

메타데이터 필터링은 검색 후 처리 단계에서 수행됩니다. FAISS는 메타데이터 기반 사전 필터링을 지원하지 않으므로, 먼저 유사도 검색을 수행한 후 결과를 필터링합니다. 이는 비효율적일 수 있지만, 본 시스템의 규모에서는 허용 가능합니다. 향후 대규모 확장 시에는 메타데이터 필터링을 지원하는 Pinecone이나 Weaviate 같은 벡터 데이터베이스로 마이그레이션을 고려할 수 있습니다.

인덱스 재구성 최소화는 중요한 최적화입니다. FAISS는 벡터 삭제를 직접 지원하지 않으므로, VectorStoreManager는 삭제 시 인덱스를 재구성합니다. 이는 비용이 큰 작업이므로 빈번하게 수행되어서는 안 됩니다. 본 시스템에서는 삭제 작업을 가능한 한 배치로 묶어 처리하고, 대량 업데이트 시에는 전체 인덱스를 한번에 재구성합니다.

메모리 관리는 대용량 인덱스에서 중요합니다. 1536차원 벡터 백만 개는 약 6GB의 메모리를 차지합니다. 시스템 메모리가 제한적인 환경에서는 인덱스를 메모리맵 방식으로 로드하여 필요한 부분만 메모리에 올릴 수 있습니다. FAISS는 read_index 대신 read_index_mmap을 사용하여 이를 지원합니다.

### 3.5. 다단계 검색 시스템

#### 3.5.1. 1차 광범위 검색 (Broad Retrieval)

다단계 검색 시스템은 사용자의 질의에 대해 점진적으로 관련성을 높여가는 전략입니다. 첫 번째 단계는 광범위 검색으로, 전체 문서 집합에서 관련 가능성이 있는 문서들을 빠르게 식별합니다.

광범위 검색은 Retrieval 클래스의 search 메서드를 통해 수행됩니다. 사용자 질의가 입력되면 먼저 동일한 임베딩 모델을 사용하여 질의를 벡터로 변환합니다. OpenAIEmbeddings 객체의 embed_query 메서드가 호출되어 1536차원의 질의 벡터가 생성됩니다.

질의 벡터는 VectorStoreManager의 search 메서드로 전달됩니다. 내부적으로 FAISS의 similarity_search_with_score 메서드가 호출되어 L2 거리 기반의 최근접 이웃 검색이 수행됩니다. top_k 파라미터에 지정된 개수만큼의 가장 유사한 청크가 반환되며, 각 청크에는 거리 점수가 함께 제공됩니다.

L2 거리는 유클리드 거리로, 두 벡터 사이의 직선 거리를 나타냅니다. 거리가 작을수록 벡터가 가까이 있어 의미가 유사함을 나타냅니다. FAISS는 거리를 제곱한 값을 반환하므로, 실제 거리는 제곱근을 취해야 하지만 순위 비교에는 영향이 없습니다.

검색 결과는 Document 객체의 리스트로 반환되며, 각 Document는 page_content와 metadata를 포함합니다. Retrieval 클래스는 이를 딕셔너리 형태로 변환하여 상위 계층에 전달합니다. 각 결과 딕셔너리에는 text, file_hash, file_name, start_page, end_page, chunk_index, chunk_hash, distance, created_at 등의 정보가 포함됩니다.

광범위 검색의 목표는 재현율을 높이는 것입니다. 관련 문서를 놓치지 않도록 충분히 많은 결과를 반환하되, 너무 많아서 후속 처리에 부담을 주지 않을 정도로 제한합니다. top_k 값은 일반적으로 5에서 20 사이로 설정되며, 본 시스템의 기본값은 5입니다.

#### 3.5.2. 2차 심층 검색 (Deep Retrieval)

광범위 검색으로 관련 문서를 식별한 후, 사용자가 더 구체적인 후속 질문을 할 수 있습니다. 예를 들어 "극저온시스템 사업"에 대해 검색한 후, "헬륨회수시스템이 사용되는 곳은?"과 같은 세부 질문을 할 수 있습니다. 이때 전체 문서에서 다시 검색하는 것보다, 1차 검색에서 식별된 특정 문서 내에서만 검색하는 것이 더 정확한 답을 제공합니다.

2차 심층 검색은 메타데이터 필터링을 활용합니다. Retrieval 클래스의 search 메서드는 filter_metadata 파라미터를 지원하며, 이를 통해 특정 file_hash를 가진 청크만 검색할 수 있습니다. 예를 들어 1차 검색에서 file_hash가 abc123인 문서가 최적 문서로 판단되었다면, 2차 검색 시 filter_metadata={'file_hash': 'abc123'}를 전달하여 해당 문서 내에서만 검색합니다.

메타데이터 필터링은 검색 후 처리 방식으로 구현됩니다. FAISS 검색이 먼저 수행되어 전체 결과를 얻은 후, 필터 조건에 맞는 결과만 선택됩니다. 이는 사전 필터링보다 비효율적이지만, FAISS가 메타데이터 기반 사전 필터링을 지원하지 않으므로 불가피합니다. 다만 본 시스템의 규모에서는 이러한 방식으로도 충분히 빠른 응답 속도를 보장합니다.

심층 검색의 효과는 실제 테스트에서 입증되었습니다. LLM 임베딩 심층검색 보고서에 따르면, "중이온 가속기 극저온시스템 스팩은?"이라는 1차 질의로 최적 문서를 식별한 후, "헬륨회수시스템이 사용되는 곳은?"이라는 2차 질의를 해당 문서 내에서 수행했을 때, 정확한 위치 정보를 포함한 상세한 답변을 얻을 수 있었습니다. 반면 관련 없는 2차 질의인 "예술경영지원센터 통합 정보시스템 구축 요건은"을 동일 문서 내에서 검색했을 때는 검색 결과가 0개로 나와, 메타데이터 필터링이 불필요한 노이즈를 효과적으로 제거함을 확인할 수 있었습니다.

#### 3.5.3. 파일 해시 기반 필터링 효과

파일 해시 기반 필터링은 다단계 검색 시스템의 핵심 메커니즘입니다. 이를 통해 검색의 정밀도를 극대화하고 사용자에게 일관된 경험을 제공할 수 있습니다.

정밀도 향상 효과는 명확합니다. 전체 문서 집합이 아닌 특정 문서 내에서만 검색하므로, 다른 문서의 유사한 표현에 의한 혼란을 방지할 수 있습니다. 예를 들어 "사업 기간"이라는 표현은 모든 제안요청서에 등장하지만, 특정 문서 내에서 검색하면 해당 사업의 기간만 정확히 찾을 수 있습니다.

컨텍스트 일관성도 보장됩니다. 대화가 진행되면서 사용자와 시스템은 특정 문서에 집중하게 됩니다. 파일 해시 필터링을 통해 이러한 컨텍스트를 유지하고, 사용자가 의도하지 않은 문서로 주제가 바뀌는 것을 방지합니다.

성능 측면의 이점도 있습니다. 필터링으로 인해 고려해야 할 청크 수가 줄어들어, 검색 후 처리 단계가 빨라집니다. 또한 LLM에 전달되는 컨텍스트가 동일 문서 내 청크들로 구성되므로 일관성이 높아져 답변 품질이 향상됩니다.

사용자 경험 측면에서는 출처의 명확성이 보장됩니다. 검색 결과가 모두 동일한 문서에서 나오므로, 사용자는 하나의 문서를 깊이 있게 탐색하고 있다는 느낌을 받습니다. 이는 물리적인 문서를 읽는 경험과 유사하여 직관적입니다.

확장 가능성도 높습니다. 파일 해시 외에 다른 메타데이터 필드를 사용한 필터링도 가능합니다. 예를 들어 발주기관, 사업 유형, 입찰 마감일 등의 필드가 추가되면, 이를 기준으로 더욱 정교한 필터링을 구현할 수 있습니다.

### 3.6. 전처리 효과 분석

#### 3.6.1. 토큰 절약 효과 (21.5%)

문서 전처리의 가장 직접적인 효과는 토큰 수 감소입니다. 전처리보고서에 따르면, 정부나라장터 제안요청서에 대해 전처리를 적용한 결과 평균 21.5퍼센트의 토큰 절약 효과가 있었습니다.

공백과 개행 정리만으로도 15.4퍼센트의 절약 효과가 있었습니다. PDF 변환 과정에서 발생한 불규칙한 공백과 과도한 빈 줄을 제거하여 원본 36,146자에서 28,359자로 감소하였습니다.

Markdown 요소 제거는 23.3퍼센트의 추가 절약을 가져왔습니다. HTML 태그, 이미지 참조, 링크 URL, 강조 기호 등 형식을 위한 마크업을 제거하여 실제 내용만 남겼습니다.

빈 테이블 행 제거는 47.4퍼센트의 극적인 효과를 보였습니다. 일부 제안요청서는 비어 있는 테이블 행을 다수 포함하고 있었는데, 이를 제거하여 큰 폭의 토큰 절약이 이루어졌습니다.

종합적으로 원본 36,146자가 전처리 후 28,359자로 감소하여 21.5퍼센트의 절약 효과가 확인되었습니다. 이는 임베딩 비용뿐만 아니라 검색 성능과 LLM 응답 생성에도 긍정적 영향을 미칩니다.

#### 3.6.2. 비용 절감 분석

토큰 절약은 직접적인 비용 절감으로 이어집니다. OpenAI의 text-embedding-3-small 모델은 백만 토큰당 0.02달러로 과금됩니다.

전처리 전 원본 텍스트는 평균 9,036 토큰으로 추정되며, 이는 문서당 0.000902달러의 임베딩 비용이 발생합니다. 전처리 후에는 평균 7,089 토큰으로 감소하여 문서당 0.000709달러가 소요됩니다. 문서당 0.000193달러의 절약 효과로 21.5퍼센트의 비용 절감이 이루어집니다.

대규모 시스템으로 확장했을 때의 효과는 더욱 명확합니다. 천 건의 문서를 처리할 경우 전처리 없이는 0.902달러가 소요되지만, 전처리를 적용하면 0.709달러로 0.193달러가 절약됩니다. 만 건의 문서에서는 1.93달러, 십만 건에서는 19.3달러의 절감 효과가 발생합니다.

월간 신규 공고가 약 6,600건이라는 점을 고려하면, 연간 약 79,200건의 문서가 처리됩니다. 전처리를 적용할 경우 연간 약 15.3달러의 임베딩 비용 절감 효과가 있습니다. 이는 작은 금액처럼 보이지만, 시스템이 확장되고 재임베딩이나 실험적 처리가 추가될 경우 절감액은 배가됩니다.

LLM 응답 생성 비용도 간접적으로 절감됩니다. 검색된 청크의 토큰 수가 줄어들면 LLM 입력 토큰 수도 감소하여 GPT 모델 사용 비용이 줄어듭니다. GPT-4o의 경우 입력 백만 토큰당 2.5달러이므로, 21.5퍼센트의 토큰 절약은 상당한 비용 효과를 가져올 수 있습니다.

#### 3.6.3. 검색 품질 개선

전처리는 비용 절감뿐만 아니라 검색 품질 향상에도 기여합니다.

노이즈 제거 효과는 명확합니다. Markdown 문법, HTML 태그, 불필요한 공백 등은 문서의 실제 의미와 무관한 노이즈입니다. 이러한 요소들이 제거되면 임베딩 모델은 순수한 내용에만 집중할 수 있어 더 정확한 의미 표현이 가능합니다.

의미 밀도가 향상됩니다. 동일한 청크 크기에 더 많은 실제 내용이 포함되어, 각 청크가 담고 있는 정보의 양이 증가합니다. 이는 검색 시 관련 정보를 찾을 확률을 높입니다.

일관성도 개선됩니다. 다양한 출처의 문서들이 서로 다른 형식과 스타일을 가지고 있지만, 전처리를 통해 통일된 형태로 변환됩니다. 이는 임베딩 공간에서 의미적으로 유사한 내용이 실제로 가까이 위치하도록 돕습니다.

False positive 감소 효과도 있습니다. 형식적 유사성으로 인한 잘못된 매칭이 줄어듭니다. 예를 들어 Markdown 링크 문법인 대괄호와 괄호가 여러 문서에 공통으로 나타나 의미 없는 유사도를 만들어내는 것을 방지합니다.

실제 테스트에서도 전처리의 효과가 확인되었습니다. 동일한 질의에 대해 전처리 전후의 검색 결과를 비교했을 때, 전처리 후 상위 결과의 관련성이 더 높았고, 거리 점수의 분포도 더 명확하게 구분되었습니다.

---
