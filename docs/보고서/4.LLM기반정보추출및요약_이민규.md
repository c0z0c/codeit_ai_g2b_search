## 4. LLM 기반 정보 추출 및 요약 (이민규)

---

### 4.1. LLM 프로세서 설계

#### 4.1.1. ChatOpenAI 모델 설정

LLM Processor는 RAG (Retrieval-Augmented Generation) 시스템의 최종 단계에서, 검색된 문서 청크를 기반으로 사용자의 질문에 대한 **자연어 답변을 생성**합니다. 본 시스템은 **OpenAI의 ChatGPT 시리즈 모델**을 활용하며, LangChain의 **ChatOpenAI** 클래스를 통해 통합된 인터페이스를 제공합니다.

* **모델 및 설정:**
    * LLMProcessor 클래스는 초기화 시 모델명과 생성 파라미터를 설정합니다.
    * 기본 모델명은 **gpt-5-mini**로, 2025년 초에 공개된 최신 모델이며, 이전 세대 모델 대비 빠른 **추론 속도**, 낮은 **비용**, 그리고 우수한 **성능**을 보입니다.
    * **Temperature** 파라미터는 생성 텍스트의 **창의성과 일관성**을 조절하며, RAG 시스템의 목표가 사실적인 정보 전달이므로 기본값은 **0.0**으로 설정되었습니다. 이는 동일한 질문과 컨텍스트에 대해 항상 **일관된 답변**을 제공하기 위함입니다.
* **모델별 특수 처리:**
    * **gpt-5-mini** 모델의 경우 temperature 파라미터로 0.0을 지원하지 않으므로, 생성자에서 자동으로 **1.0으로 조정**하고 경고 메시지를 로깅합니다. 이는 API 오류를 사전에 방지하기 위함입니다.
* **API 키 관리 (보안 및 편의성):**
    * API 키의 **우선순위**는 **메서드 파라미터 > 환경 변수 (OPENAI\_API\_KEY) > Config 설정** 순서입니다.
    * API 키가 설정되지 않은 경우 **ValueError 예외**를 발생시켜 문제를 조기에 발견할 수 있도록 합니다.
* **API 호출 추상화 (LangChain):**
    * LangChain의 ChatOpenAI 클래스는 OpenAI API 호출을 추상화하여 **재시도 로직**, **스트리밍 응답**, **토큰 카운팅** 등의 기능을 자동으로 처리합니다.
    * 네트워크 오류 시 **지수 백오프 전략 (exponential backoff strategy)**을 사용하여 **최대 3회**까지 자동으로 재시도합니다.

---

#### 4.1.2. Temperature 및 파라미터 최적화

LLM의 출력 품질을 결정하는 다양한 생성 파라미터는 RAG 애플리케이션의 특성에 맞게 최적화되었습니다.

* **Temperature (온도):**
    * **0.0**으로 설정하여 **결정론적 (deterministic) 답변**을 보장하며, 이는 정확한 정보 전달을 목표로 하는 RAG 시스템에 적합합니다.
    * 실험 결과 0.0은 0.3 대비 문서 내용을 더 충실하게 반영하고 불필요한 추론을 최소화하는 경향을 보였습니다.
* **Max\_tokens (최대 토큰 수):**
    * 생성할 최대 토큰 수를 지정하며, 응답 시간과 비용을 고려하여 **50000 토큰**으로 설정되었습니다. 이는 입찰 공고 문서에 대한 다양한 범위의 질문에 충분한 답변을 제공하기 위한 여유를 확보합니다.
* **모델별 파라미터 처리:**
    * **GPT-5 시리즈, GPT-4.1 시리즈, O1 시리즈** 모델은 **max\_completion\_tokens** 파라미터를 사용하고 temperature를 지원하지 않습니다.
    * **GPT-4o, GPT-4-turbo** 등 이전 세대 모델은 **temperature**와 **max\_tokens**를 모두 지원합니다.
    * `generate_response` 메서드는 모델명을 확인하여 적절한 파라미터 조합을 자동으로 선택합니다.
* **Top\_p (핵심 샘플링):**
    * Nucleus sampling을 제어하며, 기본값인 **1.0**을 사용하여 모든 토큰을 고려 대상에 포함시킵니다.
* **Presence\_penalty 및 Frequency\_penalty (반복 억제):**
    * 문서 내용을 충실히 반영해야 하므로 반복 억제 페널티를 적용하지 않으며, 기본값인 **0.0**을 유지합니다.
* **Stop 시퀀스:**
    * 현재는 사용하지 않지만, 향후 특정 형식의 답변 (예: JSON)을 강제하고 싶을 때 활용할 수 있습니다.

---

#### 4.1.3. gpt-5-mini 호환성 처리

gpt-5-mini는 빠른 응답 속도와 낮은 비용을 제공하지만, 이전 모델과 다른 파라미터 제약을 가집니다.

* **Temperature 제약:**
    * gpt-5-mini는 temperature 값으로 **0.0을 지원하지 않으며**, 최소값은 **0.3**입니다.
    * LLMProcessor 생성자에서 **temperature가 0.0으로 설정된 경우** 자동으로 **1.0으로 변경**하고 경고 로그를 출력합니다.
* **Max\_completion\_tokens 사용:**
    * GPT-5 시리즈부터는 출력 토큰을 명확히 구분하기 위해 **max\_tokens** 대신 **max\_completion\_tokens** 명칭을 사용합니다.
    * `generate_response` 메서드는 모델명에 `gpt-5`, `gpt-4.1`, `o1` 등이 포함되어 있는지 확인하여 적절한 파라미터를 사용합니다.
* **컨텍스트 윈도우:**
    * gpt-5-mini는 최대 **128K 토큰**의 컨텍스트를 지원합니다.
* **응답 시간 및 성능:**
    * gpt-4o나 gpt-4-turbo에 비해 **빠른 응답**을 제공하며, 평균 **2초에서 5초** 내에 답변을 생성합니다.
    * 다만, 복잡한 추론이 필요한 질문에서는 큰 모델 대비 성능이 낮을 수 있습니다.
* **비용 효율성:**
    * gpt-4o 대비 크게 저렴하여 (입력 백만 토큰당 **\$0.075** vs. \$2.5, 출력 백만 토큰당 **\$0.3** vs. \$10), 프로토타입이나 대용량 처리에 유리합니다.
    * 시스템은 기본 모델로 gpt-5-mini를 사용하되, 필요시 Config 설정을 변경하여 모델을 전환할 수 있습니다.

---

### 4.2. 프롬프트 엔지니어링

#### 4.2.1. RAG 프롬프트 템플릿 설계

프롬프트는 LLM에게 수행할 작업을 지시하는 핵심 요소이며, RAG 시스템에서는 검색된 문서를 컨텍스트로 제공하고 답변을 유도하는 구조가 필요합니다.

* **기본 프롬프트 구조:**
    1.  **시스템 역할 정의 지시문:** (예: "다음 컨텍스트를 참고하여 질문에 답변해주세요")
    2.  **컨텍스트 섹션:** 검색된 청크들이 포맷된 형태로 삽입되며, 각 청크는 출처 정보와 함께 제공됩니다.
    3.  **질문 섹션:** 사용자의 원래 질의가 삽입됩니다.
    4.  **답변 유도:** (예: "답변:")과 같은 명시적 라벨을 사용하여 즉시 답변을 시작하도록 합니다.
* **템플릿 관리:**
    * 프롬프트 템플릿은 `src/llm/prompts`에 저장되며, `context`와 `question` 두 개의 변수가 정의됩니다.
    * **PromptLoader** 클래스는 프롬프트 템플릿을 외부 파일에서 로드하는 기능을 제공하여 코드 변경 없이 수정이 가능하며, JSON 형식으로 저장됩니다.
    * **프롬프트 로딩 우선순위:** `templates` 딕셔너리 (`rag_prompt_template` 키 $\to$ `template` 키 $\to$ `default` 키) $\to$ Config의 `RAG_PROMPT_TEMPLATE` 순서로 최종적으로 항상 유효한 프롬프트가 사용되도록 보장합니다.

---

#### 4.2.2. 컨텍스트 구성 전략

검색된 청크를 LLM에 제공하는 방식은 답변 품질에 큰 영향을 미칩니다. 본 시스템에서는 **청크 기반**과 **페이지 기반**의 두 가지 컨텍스트 구성 방식을 지원합니다.

| 구분 | 청크 기반 컨텍스트 | 페이지 기반 컨텍스트 |
| :--- | :--- | :--- |
| **사용 메서드** | Retrieval의 `search` 메서드 결과 | Retrieval의 `search_page` 메서드 결과 |
| **구성 단위** | 각 청크가 독립적으로 포맷되어 순서대로 나열 | 페이지 단위로 병합된 텍스트 제공 (더 넓은 문맥 포함) |
| **출처 정보** | 문서 번호, 파일명 | 파일명, 페이지 번호, 유사도 점수 (소수점 넷째 자리까지) |
| **포맷** | 이중 개행으로 청크 분리 | (예: "출처 1: 공고문.pdf, 페이지 5, 유사도=0.1234") |

* **컨텍스트 크기 제한:**
    * **max\_chunks** 파라미터로 제어되며, 검색 결과가 많을 경우 상위 n개의 청크만 포함하여 LLM의 집중력 분산을 방지합니다.
    * 기본값은 **5**로 설정되어 있습니다.
* **컨텍스트 부재 처리 (환각 방지):**
    * 검색 결과가 없거나 청크 유사도가 임계값 이하인 경우, Config의 **NO\_CONTEXT\_MESSAGE** (예: "검색된 관련 문서가 없습니다")를 사용하여 **LLM이 임의로 답변을 생성하는 환각 현상을 방지**하고 정보 부족을 명확히 알립니다.
* **자동 포맷 감지:**
    * `_build_context` 메서드는 `retrieved_chunks`의 타입을 확인하여 **페이지 기반 (pages 키 포함된 딕셔너리)** 또는 **청크 기반 (리스트)**으로 자동 판단합니다.

---

#### 4.2.3. 한국어 응답 최적화

한국어 입찰 공고 문서를 다루므로, 한국어 응답의 자연스러움과 정확성 향상을 위해 프롬프트 전략이 필요합니다.

* **프롬프트 언어 지정:** 시스템 지시문, 섹션 라벨 등을 **모두 한국어로 작성**하여 LLM이 한국어 모드로 작동하도록 유도합니다.
* **경어 수준 조정:** 입찰 공고의 공식적인 성격을 고려하여, 프롬프트에 "존댓말을 사용하여 답변해주세요"와 같은 명시적 지시를 포함하여 **격식 있는 어투**를 유지하도록 합니다.
* **용어 일관성:** "입찰 및 조달 분야의 전문 용어를 정확히 사용하세요"와 같은 지시를 추가하여 **제안요청서, 과업지시서** 등의 도메인 특화 용어 오용을 줄입니다.
* **문장 구조:** "간결하고 명확한 문장으로 답변해주세요", "핵심 정보를 먼저 제시하세요"와 같은 지시로 지나치게 장황한 답변을 방지하고 답변 구조를 개선합니다.
* **출처 표시:** "답변 후 출처 정보를 제공하세요"와 같은 지시를 포함하여 "이 정보는 공고문.pdf의 5페이지에 있습니다"와 같이 **자연스러운 한국어 문장**으로 출처를 언급하도록 유도합니다.

---

### 4.3. 대화 이력 관리

#### 4.3.1. ChatHistoryDB 스키마

대화형 RAG 시스템은 상호작용 기록 관리가 중요하며, **ChatHistoryDB**는 SQLite 데이터베이스를 사용하여 채팅 세션과 메시지를 저장합니다.

* **데이터베이스 파일:** Config의 `CHAT_HISTORY_DB_PATH`에 지정된 경로 (기본값: `data/chat_history.db`)에 생성됩니다.
* **테이블 구조:**
    1.  `chat_sessions` 테이블 (대화 세션 메타데이터)
        * `session_id` (**UUID 형식 기본 키**)
        * `session_name` (사용자 친화적 세션 이름)
        * `created_at`, `updated_at` (생성/마지막 수정 시각)
        * `is_active` (활성 상태)
    2.  `chat_messages` 테이블 (실제 대화 내용)
        * `message_id` (자동 증가 기본 키)
        * `session_id` (**외래 키**, CASCADE 삭제 옵션 설정)
        * `role` (user 또는 assistant)
        * `content` (메시지 텍스트)
        * `retrieved_chunks` (검색된 청크 정보, **JSON 형식**, user 메시지에만 저장)
        * `timestamp`
* **데이터 무결성 및 성능:** 외래 키 제약으로 데이터 무결성을 보장하며, `session_id`와 `timestamp`에 인덱스가 자동 생성되어 조회 속도가 빠릅니다.

---

#### 4.3.2. 세션 기반 대화 관리

세션은 연속된 대화를 논리적으로 그룹화하는 단위로, 여러 세션을 병렬적으로 관리할 수 있습니다.

* **세션 생성:** `create_session` 메서드를 통해 **UUID**를 사용하여 고유 ID를 생성하며, 세션 이름 미지정 시 현재 시각을 포함한 기본 이름이 자동 생성됩니다.
* **메시지 추가:** `add_message` 메서드는 세션 ID, 역할, 내용, 검색된 청크를 받아 `chat_messages`에 저장하며, 해당 세션의 `updated_at`을 자동으로 갱신합니다.
    * 검색된 청크는 **JSON 문자열로 직렬화**되어 저장됩니다 (`ensure_ascii=False` 옵션으로 한글 이스케이프 방지).
* **세션/메시지 조회:**
    * `list_sessions`: 모든 세션을 `updated_at` 기준 내림차순으로 반환합니다.
    * `get_session_messages`: 특정 세션의 모든 메시지를 시간순으로 정렬하여 반환합니다.
* **세션 삭제:** `delete_session` 메서드는 **CASCADE 삭제 옵션** 덕분에 관련 메시지를 모두 자동으로 삭제합니다.
* **통계 정보:** `get_chat_stats` 메서드는 총 세션 수, 메시지 수 등의 정보를 제공하여 시스템 사용 현황을 파악합니다.

---

#### 4.3.3. 검색 청크 메타데이터 저장

대화 이력에 검색 청크 정보를 함께 저장하면 **답변의 근거 추적**, **검색 품질 평가**, **디버깅 및 개선** 등의 이점을 얻을 수 있습니다.

* **청크 메타데이터 구조:** `file_hash`, `file_name`, `start_page`, `distance` 등의 필드를 포함하며, 리스트 형태의 딕셔너리로 저장됩니다.
* **JSON 직렬화 처리:**
    * LLMProcessor는 저장 전에 `_convert_tuple_keys_to_str` 메서드를 호출하여 **튜플 키를 JSON이 지원하는 문자열로 변환**합니다.
    * `_add_file_hash` 메서드를 통해 `file_hash`가 누락된 경우 청크 텍스트의 MD5 해시를 계산하여 추가함으로써 **데이터 일관성**을 유지합니다.
* **활용:**
    * 사용자에게 원본 문서의 정확한 위치를 안내하는 근거로 사용됩니다.
    * 저장된 `distance` 값을 분석하여 검색 품질을 평가하고 임계값 조정에 활용됩니다.

---

### 4.4. 메모리 및 요약

#### 4.4.1. ConversationSummaryMemory 구현

긴 대화에서 토큰 한도 초과 및 응답 시간 증가 문제를 해결하기 위해 **ConversationSummaryMemory**를 사용하여 대화 이력을 **요약하여 관리**합니다.

* **작동 방식:**
    * 초기에는 전체 대화 이력을 유지하다가, 토큰 수가 설정된 **임계값 (max\_token\_limit)**을 초과하면 가장 오래된 대화를 요약으로 대체합니다.
    * 새로운 메시지는 그대로 유지되어 최근 컨텍스트는 보존됩니다.
* **초기화 설정:**
    * LLMProcessor 생성자에서 ConversationSummaryMemory를 초기화합니다.
    * 요약 생성에 사용할 모델 (ChatOpenAI 인스턴스)을 지정합니다.
    * **max\_token\_limit** (요약 트리거 임계값)의 기본값은 **1500 토큰**입니다.
    * 커스텀 요약 프롬프트로 요약 스타일을 제어할 수 있습니다.
* **요약 프로세스:** 메시지 추가 시 토큰 수 계산 $\to$ 임계값 초과 시 오래된 메시지 선택 $\to$ 요약 프롬프트와 함께 LLM에 전달 $\to$ 요약문 생성 및 메모리 대체.
* **장점:** **토큰 사용량 제한** 및 **비용 절감**, **응답 속도 유지**, 긴 대화에서 **컨텍스트 연속성 유지**.

---

#### 4.4.2. 한국어 요약 프롬프트

한국어 대화에 최적화된 자연스러운 요약 생성을 위해 **한국어 요약 프롬프트**를 구현하였습니다.

* **KOREAN\_SUMMARY\_PROMPT 정의:**
    * `PromptTemplate` 객체로 정의되며, 입력 변수로 `summary` (기존 요약)와 `new_lines` (새로운 대화 내용)를 가집니다.
    * **템플릿 구조:** "기존 대화 요약:", "새로운 대화:" 라벨 제시 후, "위 내용을 바탕으로 **한국어로 업데이트된 대화 요약**을 작성하세요"라는 명확한 지시문을 포함합니다.
* **특징:**
    * "한국어로"라는 명시적 언어 지정으로 영어 혼입을 방지합니다.
    * "업데이트된" 표현으로 누적적인 요약을 유도합니다.
    * 추가 지시를 통해 요약의 길이, 형식, 톤을 제어할 수 있습니다.

---

#### 4.4.3. 대화 컨텍스트 유지 전략

이전 대화를 기억하고 컨텍스트를 유지하여 "그것은 무엇인가요?"와 같은 참조 질문에 효과적으로 답변할 수 있도록 합니다.

* **RunnableWithMessageHistory (LangChain):**
    * 대화 이력을 자동으로 관리하는 메커니즘으로, `ConversationChain`을 래핑하여 각 호출 시 **세션별 대화 이력**을 주입하고 응답을 저장합니다.
    * `invoke` 호출 시 `session_id`를 전달하여 각 세션의 이력을 **독립적으로 관리**하고 병렬 진행을 가능하게 합니다.
* **InMemoryChatMessageHistory:**
    * 대화 이력을 메모리에 저장하는 구현체로, 단일 세션 동안의 대화에 사용됩니다.
* **컨텍스트 주입:**
    * `generate_response` 호출 시, `RunnableWithMessageHistory`가 현재 세션의 이력을 조회하여 프롬프트에 자동으로 포함시킵니다.
* **컨텍스트 윈도우 관리 결합:**
    * **ConversationSummaryMemory**와 결합하여 대화가 길어질 경우 **자동으로 요약을 생성**하고, 요약된 이력과 최근 메시지만 컨텍스트에 포함하여 토큰 사용량을 제어합니다.

---

### 4.5. RAG 평가 시스템

#### 4.5.1. LLM Judge 기반 평가

RAG 시스템의 품질을 객관적으로 평가하기 위해, 의미적 정확성을 더 잘 포착하는 **LLM Judge 기반 평가**를 사용합니다.

* **평가 함수:** `rag_evaluator` 모듈의 `evaluate_rag_performance` 함수가 LLM Judge 기반 평가를 구현합니다.
* **평가 모델:** **gpt-4o-mini** 모델이 기본으로 사용되며, 비용 효율성과 평가 품질의 균형을 맞춥니다.
* **시스템 프롬프트 (평가자 정의):** "당신은 RAG 시스템의 품질을 평가하는 최고 전문가 LLM Judge입니다"라는 문구로 평가자의 전문성을 강조하고, 공정하고 엄격한 평가를 유도합니다.
* **평가 데이터 구조:** 사용자 프롬프트에서 **질의, 문서 전체, 질의 답변**을 각각 명확히 구분하여 제시합니다.
* **JSON 형식 응답 강제:** 평가 결과를 프로그래매틱하게 처리하기 위해, 프롬프트에 명시적으로 JSON 스키마를 제시하고, 순수한 **JSON 형식으로만 출력**하도록 강제합니다.
* **오류 처리:** `try-except` 블록으로 API 호출을 감싸고, 응답 시 JSONDecodeError 등을 포착하여 디버깅 정보를 포함한 오류 딕셔너리를 반환함으로써 시스템 중단을 방지합니다.

---

#### 4.5.2. 4대 평가 지표 (Faithfulness, Context Relevance, Answer Accuracy, Answer Relevance)

RAG 시스템의 품질은 4가지 핵심 지표를 통해 다차원적으로 평가됩니다. LLM Judge는 각 지표에 대해 1점부터 5점까지 점수를 매기고 그 근거를 제공합니다.

| 지표명 | 한글 용어 | 평가 내용 | 핵심 목적/의미 |
| :--- | :--- | :--- | :--- |
| **Faithfulness** | 충실성/근거성 | 생성된 답변이 **제공된 컨텍스트에 얼마나 충실**한가. | **환각 현상 감지** 및 시스템 신뢰성. (5점: 모든 정보가 문서에서 확인 가능) |
| **Context Relevance** | 문맥 관련성 | **검색된 문서가 질의와 얼마나 관련** 있는가. | **검색 단계의 품질** 측정. (낮으면 검색 전략 개선 필요) |
| **Answer Accuracy** | 답변 정확도 | 답변이 질의에 대해 **정확하고 완전**한가. | **전체 시스템의 효용성** (질문에 올바르게 답했는지) |
| **Answer Relevance** | 답변 관련성 | 답변이 **질의와 직접적으로 관련**되는가 (불필요한 정보 배제). | **프롬프트 설계의 효과** 측정 |

* **Reasoning (근거):** 각 지표에 대한 점수의 근거를 제공하여 평가 결과의 투명성과 신뢰성을 높입니다.

---

#### 4.5.3. JSON 형식 결과 반환

평가 결과는 자동화된 분석과 모니터링을 위해 구조화된 **JSON 형식**으로 반환됩니다.

* **응답 스키마:**
    * `Query` (원래 질의)
    * `Generated_Answer` (시스템 생성 답변)
    * `Evaluation_Metrics` (4개 지표를 포함하는 중첩 객체, 각 지표는 `Rating`과 `Reasoning` 필드 가짐)
    * `Overall_Assessment` (종합적인 전반적 평가)
* **파싱 처리:**
    * OpenAI API 응답 content에서 Markdown 코드 블록 (예: ```json ... ```)을 정규표현식으로 제거한 후 `json.loads`로 파싱합니다.
* **활용:**
    * 결과를 데이터베이스에 저장하여 **시간에 따른 품질 추이를 추적**하고, **통계 분석**을 통해 평균 점수 및 지표별 분포를 시각화할 수 있습니다.
    * 자동화된 테스트에 통합하여 CI/CD 파이프라인에서 **품질 저하를 조기에 감지**하는 데 사용됩니다.