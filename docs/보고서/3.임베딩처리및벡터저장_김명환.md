## 3. 임베딩 처리 및 벡터 저장 (김명환)

### 3.1. 임베딩 전처리 전략 (Embedding Preprocessing Strategy)

RAG (Retrieval-Augmented Generation) 시스템의 성능을 결정짓는 핵심 요소 중 하나는 문서 전처리의 품질입니다. 본 시스템은 **원본 보존**과 **처리 최적화**라는 두 가지 목표를 균형 있게 달성하기 위해 3단계 전처리 파이프라인을 설계하였습니다.

#### 3.1.1. 3단계 전처리 파이프라인 (Three-Stage Preprocessing Pipeline)

| 단계 | 수행 시점 | 주체 (메서드) | 목표 | 주요 작업 | 저장 위치 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **1차 (최소)** | PDF 변환 직후 | DocumentProcessor.clean_markdown_text | 원본 보존 | 형식 문제 해결 (공백, 개행 축소) | DocumentsDB.text_content |
| **2차 (최대)** | 청킹 직전 | EmbeddingProcessor.clean_markdown_text | 임베딩 품질 최적화 | 공격적인 마크업/노이즈 제거, 보호 블록 마스킹/복원 | 임베딩 입력 |
| **3차 (페이지별)** | 페이지 단위 분할 후 | EmbeddingProcessor.clean_page_text | 최종 정제 | 페이지 마커 제거, 최종 공백 정리 | 청크 텍스트 |

이 3단계 파이프라인 구조는 다음과 같은 장점을 제공합니다:

* **원본 보존:** 원본 데이터는 최소 전처리만 거친 상태로 보존되어 다른 전처리 방식 실험 시 시작점으로 활용 가능합니다.
* **유연성:** 최대 전처리는 임베딩 직전에 수행되므로 설정 변경 시 PDF 변환을 다시 할 필요가 없습니다.
* **기능 분리:** 페이지별 정제는 청킹 이후에 수행되므로 페이지 마커를 활용한 정확한 분할이 가능합니다.

#### 3.1.2. 보호 블록 마스킹 기법 (Protected Block Masking Technique)

Markdown 전처리 과정에서 코드 예제, 수학 공식, 다이어그램 등 중요한 정보가 손실되는 것을 방지하기 위해 **보호 블록 마스킹 (Masking)** 기법이 사용됩니다.

1.  **식별 및 치환:** 정규표현식을 사용하여 보호할 블록을 식별하고, 각 블록을 고유한 플레이스홀더로 치환합니다.
    * **코드 블록:** 4개 백틱 (`...`) 또는 3개 백틱 (`...`) 블록을 `XPROTECTEDXCODE4XnX` 또는 `XPROTECTEDXCODE3XnX` 형식의 플레이스홀더로 치환합니다.
    * **수식 블록:** 이중 달러 기호 (`$$...$$`)의 블록 수식 및 단일 달러 기호 (`$...$`)의 인라인 수식을 각각 `XPROTECTEDXMATHXnX`, `XPROTECTEDXINLINEXnX` 형식으로 치환합니다.
    * **페이지 마커:** `ERROR_PAGE_MARKER`, `EMPTY_PAGE_MARKER`, 페이지 번호 마커를 `XPROTECTEDXMARKERXnX` 형식으로 치환하여 페이지 분할 단계까지 안전하게 전달합니다.
2.  **전처리 수행:** 마스킹 이후 일반적인 전처리 작업이 수행됩니다. 플레이스홀더는 영문자와 숫자로만 구성되어 전처리 작업에 영향을 받지 않습니다.
3.  **복원:** 전처리가 완료되면 `protected_blocks` 및 `protected_markers` 딕셔너리를 역순으로 순회하며 플레이스홀더를 원본 내용으로 치환합니다. 역순 처리는 중첩 블록 및 인덱스 충돌을 방지합니다.

#### 3.1.3. Markdown 요소 제거 및 정제 (Markdown Element Removal and Normalization)

보호 블록 마스킹 완료 후, 문서의 의미를 담고 있는 텍스트만 남기기 위해 형식을 위한 마크업을 제거하고 텍스트를 정제합니다.

주요 정제 작업은 다음과 같습니다:

* **탈출문자 처리:** 백슬래시로 이스케이프된 특수 문자를 원래 문자로 복원.
* **HTML 태그 제거:** `<[^>]+>` 패턴으로 `<사업<div>개요</div>>`를 `사업 개요`와 같이 공백으로 치환.
* **이미지 제거:** Markdown 이미지 문법 `![대체텍스트](URL)`을 찾아 삭제.
* **링크 제거:** `[나라장터](https://g2b.go.kr)`를 `나라장터`로 변환하여 URL만 제거하고 텍스트 유지.
* **강조 기호 제거:** 볼드체 (`**...**`), 이탤릭체 (`*...*`), 취소선 (`~~...~~`)의 마크업 제거.
* **헤더 제거:** `#` 기호와 뒤따르는 공백 제거 (예: `## 사업 개요` $\rightarrow$ `사업 개요`).
* **인용구 제거:** 라인 시작 부분의 꺾쇠 기호 (`>`) 제거.
* **리스트 마커 제거:** 순서 있는/없는 리스트의 마커 제거.
* **공백 정리:** 연속된 세 줄 이상의 개행을 두 줄로 축소, 연속된 공백/탭을 단일 공백으로 변환, 빈 테이블 행 및 긴 점선 축약.

이러한 정제 작업의 결과로 문서는 순수한 텍스트 형태가 되어 임베딩 모델 (embedding model)이 의미에 집중할 수 있게 됩니다.

---

### 3.2. 청킹 전략 (Chunking Strategy)

문서를 적절한 크기의 청크 (chunk)로 분할하는 것은 RAG 시스템에서 검색 정밀도와 문맥 유지의 균형을 맞추는 핵심 과제입니다.

#### 3.2.1. RecursiveCharacterTextSplitter 설정

LangChain의 **RecursiveCharacterTextSplitter**를 사용하여 균형 잡힌 청킹을 구현했습니다. 이 분할기는 큰 구분자부터 작은 구분자로 재귀적으로 텍스트를 분할합니다.

* **구분자 (Separators):** 이중 개행 (`\n\n`), 단일 개행 (`\n`), 공백 (` `), 빈 문자열 (` `) 순서로 우선순위가 정의되어 문단 $\rightarrow$ 문장 $\rightarrow$ 단어 $\rightarrow$ 문자 단위 분할을 시도합니다.
* **청크 크기 ($chunk\_size$):** 기본값은 **1500**입니다. `CHUNKING_MODE`가 `token`으로 설정되어 토큰 수 (token count)를 기준으로 합니다.
* **청크 중복 ($chunk\_overlap$):** 기본값은 **300**입니다. 인접 청크 간 중복을 허용하여 청크 경계에서의 문맥 단절을 방지합니다.
* **길이 함수 ($length\_function$):** 토큰 기반 청킹을 위해 `tiktoken` 라이브러리를 사용하는 커스텀 함수를 제공하여 정확한 토큰 수 측정을 보장합니다.

#### 3.2.2. 페이지 단위 분할 방식 (Page-Based Splitting Method)

청킹 과정에서 페이지 경계를 고려하여 출처 추적 (source tracking) 및 사용자 경험을 개선합니다.

1.  **페이지 분리:** DocumentProcessor가 삽입한 페이지 마커 (예: `--- 페이지 1 ---`)를 기준으로 전체 텍스트를 페이지 단위 텍스트 리스트로 분할합니다.
2.  **버퍼 누적:** 단일 페이지가 `CHUNK_SIZE`보다 작은 경우가 많으므로, **버퍼 (Buffer)**를 사용하여 인접 페이지의 텍스트를 누적하고 시작/종료 페이지 번호를 기록합니다.
3.  **청킹 트리거:** 버퍼 크기가 `CHUNK_SIZE` 이상이 되거나 마지막 페이지에 도달하면 청킹이 트리거됩니다.
    * **단일/병합 청크:** 버퍼 토큰 수가 `CHUNK_SIZE` 이하인 경우 버퍼 전체를 단일 청크로 추가하고 `chunk_type`을 `single` 또는 `merged`로 설정합니다.
    * **분할 청크:** 버퍼 토큰 수가 `CHUNK_SIZE`를 초과하면 RecursiveCharacterTextSplitter를 사용하여 버퍼를 분할하고 `chunk_type`을 `split`으로 설정합니다.
4.  **페이지 범위 기록:** 각 청크의 메타데이터에 `start_page`와 `end_page` 필드를 저장하여 검색 결과를 사용자에게 제시할 때 출처를 명시합니다.

#### 3.2.3. 메타데이터 보존 전략 (Metadata Preservation Strategy)

각 청크에는 검색 및 추적을 용이하게 하는 풍부한 메타데이터 (metadata)가 첨부됩니다.

| 메타데이터 필드 | 설명 | 활용 목적 |
| :--- | :--- | :--- |
| **file\_hash** | 원본 파일의 SHA-256 해시값 | 특정 파일의 청크 식별, 파일 단위 벡터 삭제 |
| **file\_name** | 사용자 친숙한 파일 이름 | 검색 결과 표시 |
| **start\_page/end\_page** | 청크가 속한 페이지 범위 | 원본 문서의 정확한 위치 추적 |
| **chunk\_type** | 청크 생성 방식 (`single`, `merged`, `split`) | 청킹 전략 분석 및 개선 |
| **chunk\_index** | 동일 파일 내 청크의 순서 인덱스 | 청크 정렬, 인접 청크 검색 |
| **embedding\_config\_hash** | 파일 해시와 임베딩 설정의 통합 해시값 | **재현성 보장 및 재임베딩 필요성 자동 판단** |
| **chunk\_hash** | 청크 텍스트 내용의 SHA-256 해시값 | 증분 업데이트 시 내용 변경 감지 |
| **config\_\*** | 청크 생성 당시의 설정값들 | 나중에 동일한 조건으로 청크 재생성 |
| **embedding\_version** | 사용된 임베딩 모델 이름 | 모델 변경 시 추적 및 마이그레이션 |
| **created\_at** | 청크 생성 시각 (ISO 8601) | 시간 기반 필터링 및 데이터 갱신 분석 |

메타데이터는 JSON 직렬화가 가능한 형태로 저장되어 LangChain의 Document 객체에 포함되며, VectorStoreManager는 이를 FAISS 인덱스와 함께 저장하고 검색 시 반환합니다.

---

### 3.3. 벡터 임베딩 (Vector Embedding)

벡터 임베딩은 텍스트를 고차원 벡터 공간의 점으로 변환하여 의미가 유사한 텍스트가 가까운 위치에 놓이게 하는 과정입니다.

#### 3.3.1. OpenAI text-embedding-3-small 모델

본 시스템에서는 OpenAI의 최신 임베딩 모델인 **text-embedding-3-small**을 사용합니다.

| 특징 | 설명 |
| :--- | :--- |
| **벡터 차원** | 1536차원 |
| **비용 효율성** | 백만 토큰당 0.02달러 (이전 모델 대비 5배 저렴) |
| **처리 속도** | 빠른 API 응답 시간, 배치 처리 지원 |
| **다국어 지원** | 한국어를 포함한 100개 이상의 언어 지원 |
| **컨텍스트 길이** | 최대 8191 토큰 처리 가능 |

임베딩 생성 프로세스는 LangChain의 `OpenAIEmbeddings` 클래스를 통해 추상화되며, `EMBEDDING_BATCH_SIZE` (기본값 100) 설정에 따라 여러 청크를 하나의 API 호출로 처리하는 **배치 처리 (Batch Processing)**를 사용하여 효율성을 높입니다.

#### 3.3.2. 임베딩 설정 해시 (embedding\_config\_hash)

임베딩 벡터의 재현성 (reproducibility)을 보장하고 설정 변경을 추적하기 위해 **`embedding_config_hash`** 메커니즘이 도입되었습니다. 이는 파일 해시와 임베딩 관련 모든 설정을 결합하여 계산한 SHA-256 해시값입니다.

$$
\text{embedding\_config\_hash} = \text{SHA-256}(\text{Sort}(\text{file\_hash}, \text{chunk\_size}, \text{chunk\_overlap}, \text{embedding\_model}, \dots))
$$

* **재현성 보장:** 청크 내용, 임베딩 모델, 청킹/전처리 설정 등 벡터 생성에 영향을 미치는 모든 요소가 포함되므로, 설정이 변경되면 해시값이 달라져 재임베딩이 필요함을 자동 판단합니다.
* **활용:** VectorStoreManager는 이 값을 비교하여 중복 방지, 자동 재임베딩, 실험 추적 등에 사용합니다.

#### 3.3.3. 재현성 보장 메커니즘 (Reproducibility Mechanism)

| 재현성 수준 | 핵심 메커니즘 | 보장 내용 |
| :--- | :--- | :--- |
| **파일 수준** | file\_hash | 동일한 PDF 파일 $\rightarrow$ 동일한 해시값 및 `text_content` |
| **설정 수준** | embedding\_config\_hash | 동일한 설정 $\rightarrow$ 동일한 청크 생성 (Config 파일로 설정 복원 가능) |
| **청크 수준** | chunk\_hash | 동일한 텍스트 $\rightarrow$ 동일한 임베딩 벡터 (OpenAI 모델은 결정론적) |
| **메타데이터 수준** | config\_\* 필드 | 청크 생성 당시의 설정을 완전히 기록 |
| **버전 관리** | embedding\_version | 임베딩 모델 업데이트 시 재임베딩 필요성 판단 |

---

### 3.4. FAISS 벡터 저장소 (FAISS Vector Store)

FAISS (Facebook AI Research Similarity Search)는 고성능 벡터 유사도 검색 라이브러리로 대규모 RAG 시스템에 적합합니다.

#### 3.4.1. 인덱스 구조 및 관리 (Index Structure and Management)

* **인덱스 타입:** **IndexFlatL2** 인덱스를 사용합니다. 이는 L2 거리 (Euclidean distance) 기반의 정확한 최근접 이웃 검색 (Exact Nearest Neighbor Search)을 수행하여 검색 결과의 정확도를 보장합니다.
* **LangChain 래퍼:** LangChain의 `FAISS` 클래스를 사용하여 FAISS 인덱스 외에도 **`docstore`** (Document 객체 저장 딕셔너리)와 **`index_to_docstore_id`** (FAISS 인덱스와 docstore ID 매핑)를 관리하여 벡터와 원본 문서를 연결합니다.
* **저장 및 로드:** `save_local` 및 `load_local` 메서드를 통해 FAISS 인덱스 (`vectorstore.faiss`, 바이너리)와 메타데이터 (`vectorstore.pkl`, pickle)를 저장 및 복원합니다.

#### 3.4.2. 메타데이터 매핑 전략 (Metadata Mapping Strategy)

FAISS 자체는 메타데이터를 지원하지 않으므로, 효율적인 메타데이터 관리를 위해 **`chunk_map`**이라는 추가 자료구조를 도입합니다.

$$
\text{chunk\_map} = \text{Dict}[(\text{file\_hash}, \text{chunk\_index}), (\text{faiss\_idx}, \text{chunk\_hash}, \text{embedding\_config\_hash})]
$$

`chunk_map`의 주요 활용 목적:

* **빠른 중복 검사:** $O(1)$ 시간 복잡도로 새로운 청크의 존재 여부를 즉시 확인.
* **내용 변경 감지:** `chunk_hash` 비교를 통해 문서 업데이트 시 변경된 청크만 재처리.
* **설정 변경 감지:** `embedding_config_hash` 비교를 통해 설정 변경 시 재임베딩 필요성 판단.
* **효율적인 삭제:** 특정 `file_hash`를 가진 항목들의 FAISS 인덱스를 빠르게 수집하고 일괄 삭제.

#### 3.4.3. 검색 성능 최적화 (Search Performance Optimization)

* **인덱스 타입 선택:** 현재는 정확도를 위한 IndexFlatL2를 사용하지만, 향후 대규모 확장 시 **IndexIVFFlat** 또는 **IndexHNSW**와 같은 근사 검색 인덱스 (Approximate Nearest Neighbor Search)로 전환을 고려할 수 있습니다.
* **top\_k 조정:** 필요 이상으로 큰 값을 설정하지 않아 검색 시간을 최소화합니다 (기본값 5).
* **메타데이터 필터링:** FAISS는 사전 필터링을 지원하지 않아 유사도 검색 후 결과를 필터링합니다. 대규모 확장 시 Pinecone, Weaviate 등 메타데이터 필터링을 지원하는 벡터 데이터베이스 마이그레이션을 고려할 수 있습니다.
* **인덱스 재구성 최소화:** FAISS의 벡터 삭제 비용이 크므로, 삭제 작업을 배치 (batch)로 묶어 처리하고 대량 업데이트 시에만 인덱스를 재구성합니다.

---

### 3.5. 다단계 검색 시스템 (Multi-Stage Retrieval System)

사용자 질의에 대해 점진적으로 관련성을 높여가는 전략을 사용하여 검색의 정밀도를 향상시킵니다.

#### 3.5.1. 1차 광범위 검색 (Broad Retrieval)

1차 검색은 전체 문서 집합에서 관련 가능성이 있는 청크를 빠르게 식별하여 재현율 (Recall)을 높이는 것을 목표로 합니다.

1.  **질의 임베딩:** 사용자 질의를 OpenAIEmbeddings를 사용하여 1536차원 벡터로 변환합니다.
2.  **FAISS 검색:** FAISS의 `similarity_search_with_score`를 호출하여 L2 거리 기반의 최근접 이웃 검색을 수행하고, `top_k` 개수 (기본값 5)만큼의 청크와 거리 점수를 반환합니다.
3.  **결과 반환:** 청크의 텍스트와 메타데이터 (file\_hash, start\_page, distance 등)를 포함하는 리스트를 반환합니다.

#### 3.5.2. 2차 심층 검색 (Deep Retrieval)

1차 검색에서 식별된 특정 문서 내에서만 검색을 수행하여 정밀도 (Precision)를 높입니다.

* **메타데이터 필터링 활용:** `filter_metadata={'file_hash': 'abc123'}`와 같이 특정 `file_hash`를 가진 청크만 검색하도록 조건을 전달합니다.
* **검색 후 처리:** FAISS가 사전 필터링을 지원하지 않으므로, 1차 검색 후 결과에 대해 필터 조건을 적용하여 최종 결과를 선택합니다.
* **효과:** "헬륨회수시스템이 사용되는 곳은?"과 같은 세부 질문에 대해 관련 문서 내에서만 검색함으로써 정확한 답변과 위치 정보를 얻을 수 있습니다. 불필요한 노이즈를 효과적으로 제거합니다.

#### 3.5.3. 파일 해시 기반 필터링 효과 (File Hash Based Filtering Effect)

파일 해시 기반 필터링은 2차 심층 검색의 핵심 메커니즘입니다.

* **정밀도 향상:** 전체 문서가 아닌 특정 문서 내에서만 검색하므로, 다른 문서의 유사 표현에 의한 잘못된 매칭 (false positive)을 방지합니다.
* **컨텍스트 일관성:** 대화가 특정 문서에 집중되도록 컨텍스트를 유지하고, LLM에 전달되는 컨텍스트의 일관성을 높여 답변 품질을 향상시킵니다.
* **사용자 경험:** 검색 결과가 모두 동일한 문서에서 나오므로, 사용자는 하나의 문서를 깊이 있게 탐색하는 직관적인 경험을 얻을 수 있습니다.

---

### 3.6. 전처리 효과 분석 (Preprocessing Effect Analysis)

#### 3.6.1. 토큰 절약 효과 (21.5%)

정부나라장터 제안요청서에 대한 전처리 적용 결과, 평균 **21.5%**의 토큰 절약 효과가 확인되었습니다.

* **공백/개행 정리:** 15.4% 절약 (불규칙한 공백/과도한 빈 줄 제거).
* **Markdown 요소 제거:** 23.3% 추가 절약 (HTML 태그, 링크, 강조 기호 등 형식 마크업 제거).
* **빈 테이블 행 제거:** 47.4%의 극적인 효과.

종합적으로 원본 36,146자 $\rightarrow$ 전처리 후 28,359자로 감소.

#### 3.6.2. 비용 절감 분석 (Cost Reduction Analysis)

토큰 절약은 직접적인 비용 절감으로 이어집니다. text-embedding-3-small 모델은 백만 토큰당 0.02달러로 과금됩니다.

| 항목 | 전처리 전 (평균) | 전처리 후 (평균) | 절감 효과 |
| :--- | :--- | :--- | :--- |
| **토큰 수 (문서당)** | 9,036 토큰 (추정) | 7,089 토큰 (추정) | 21.5% |
| **임베딩 비용 (문서당)** | 0.000902 달러 | 0.000709 달러 | 0.000193 달러 |
| **연간 절감액 (79,200건)** | 71.49 달러 | 56.15 달러 | 약 15.3 달러 |

또한, 검색된 청크의 토큰 수 감소는 LLM (Large Language Model) 입력 토큰 수를 줄여 **LLM 응답 생성 비용도 간접적으로 절감**하는 효과를 가져옵니다 (예: GPT-4o 입력 비용 감소).

#### 3.6.3. 검색 품질 개선 (Search Quality Improvement)

전처리는 노이즈 제거를 통해 검색 품질 향상에 기여합니다.

* **노이즈 제거:** Markdown 문법, HTML 태그, 불필요한 공백 등이 제거되어 임베딩 모델이 **순수한 내용에만 집중**할 수 있어 더 정확한 의미 표현이 가능해집니다.
* **의미 밀도 향상:** 동일 청크 크기에 더 많은 실제 내용이 포함되어 정보의 양이 증가하고 관련 정보를 찾을 확률이 높아집니다.
* **일관성 개선:** 다양한 문서 형식이 통일된 형태로 변환되어 임베딩 공간에서 의미적 유사성을 높입니다.
* **False Positive 감소:** 형식적 유사성으로 인한 잘못된 매칭이 줄어듭니다.

실제 테스트에서 전처리 후 상위 검색 결과의 관련성이 더 높고 거리 점수 분포가 더 명확하게 구분되는 것이 확인되었습니다.

