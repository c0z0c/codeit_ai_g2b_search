# -*- coding: utf-8 -*-
from typing import List, Dict, Any, Optional
from getpass import getpass
import os

try:
    # 1. OpenAI í†µí•© ëª¨ë“ˆ
    from langchain_openai import ChatOpenAI
    
    # 2. Core ëª¨ë“ˆ
    from langchain_core.chat_history import InMemoryChatMessageHistory
    from langchain_core.runnables.history import RunnableWithMessageHistory
    from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
    
    # 3. Memory ë° Chain ëª¨ë“ˆ (0.2.x ë²„ì „ì˜ í‘œì¤€ ê²½ë¡œ)
    # â­ï¸ .memory ëŒ€ì‹  .chainsì—ì„œ ì„í¬íŠ¸ ì‹œë„ (v0.2.x êµ¬ì¡°)
    from langchain.chains import ConversationChain 
    
    # â­ï¸ MemoryëŠ” v0.2.xì—ì„œ langchain íŒ¨í‚¤ì§€ì— ìœ„ì¹˜í•˜ì§€ë§Œ, ê²½ë¡œ ì¶©ëŒ ë°©ì§€ ìœ„í•´ ìœ ì§€
    # Note: v0.2.xì—ì„œ ConversationSummaryMemoryëŠ” langchain.memoryì— ìˆìŠµë‹ˆë‹¤.
    from langchain.memory import ConversationSummaryMemory 
    
    # 4. ê¸°íƒ€ Chain (LLMChainì€ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì œê±°)
    
    LANGCHAIN_AVAILABLE_LLM = True
    # LLMProcessorì—ì„œ ì‚¬ìš©í•˜ëŠ” ë³€ìˆ˜ ëª… í†µì¼
    LANGCHAIN_AVAILABLE = LANGCHAIN_AVAILABLE_LLM 
    
except ImportError as e:
    LANGCHAIN_AVAILABLE_LLM = False
    LANGCHAIN_AVAILABLE = LANGCHAIN_AVAILABLE_LLM 
    print("\n=============================================")
    print(f"ğŸš¨ LangChain ì„í¬íŠ¸ ì‹¤íŒ¨! (LANGCHAIN_AVAILABLE_LLM = False)")
    print(f"ğŸš¨ ì˜¤ë¥˜ ë©”ì‹œì§€: {e}")
    print("=============================================\n")

from src.llm.prompts.prompt_loader import PromptLoader

# from openai import OpenAI
from src.config import get_config
from src.utils.logging_config import get_logger

class LLMProcessor:
    """
    LLMProcessor í´ë˜ìŠ¤ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•˜ì—¬ ì‚¬ìš©ì ì…ë ¥ê³¼ ê²€ìƒ‰ëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ
    ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ê³¼ LLM í˜¸ì¶œì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.

    ì£¼ìš” ê¸°ëŠ¥:
    - ì„¤ì •(config) ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ ë° ì˜¨ë„(temperature) ì´ˆê¸°í™”
    - ê²€ìƒ‰ëœ ì²­í¬ ë°ì´í„°ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    - LangChainì„ í†µí•´ LLM ì‘ë‹µ ìƒì„±
    """

    def __init__(    
        self,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        config=None,
        api_key: Optional[str] = None,
        prompt_file: Optional[str] = None
    ):
        """
        LLMProcessor ì´ˆê¸°í™”

        Args:
            model: LLM ëª¨ë¸ëª…
            temperature: ìƒì„± ì˜¨ë„
            config: ì„¤ì • ê°ì²´
        """
         
        # Config & Logger
        self.config = config or get_config()
        self.logger = get_logger(__name__)

        self.model_name = model or self.config.OPENAI_MODEL
        self.temperature = temperature if temperature is not None else self.config.OPENAI_TEMPERATURE

        # íŠ¹ì • ëª¨ë¸ì˜ temperature ì œí•œ íšŒí”¼
        if self.model_name == 'gpt-5-mini' and self.temperature == 0.0:
            self.logger.warning("gpt-5-miniëŠ” temperature=0.0ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. 1.0ìœ¼ë¡œ ê°•ì œ ë³€ê²½í•©ë‹ˆë‹¤.")
            self.temperature = 1.0

        # API í‚¤ ì…ë ¥ ì²˜ë¦¬
        if api_key is None or not api_key.strip():
            api_key_input = getpass("OpenAI API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            api_key = api_key_input if api_key_input else None

        if not api_key:
            raise ValueError("OpenAI API Keyê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # í™˜ê²½ ë³€ìˆ˜ì— ë“±ë¡ (LangChainì—ì„œ ìë™ ì‚¬ìš©)
        os.environ["OPENAI_API_KEY"] = api_key

        # LangChain ì´ˆê¸°í™”
        if LANGCHAIN_AVAILABLE:
            self.llm = ChatOpenAI(model=self.model_name, temperature=self.temperature)
            self.logger.info(f"LLMProcessor ì´ˆê¸°í™” ì™„ë£Œ (model={self.model_name}, temperature={self.temperature})")
        else:
            self.logger.error("LangChainì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            raise ImportError("LangChainì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # í”„ë¡¬í”„íŠ¸ ë¡œë” ì´ˆê¸°í™”
        self.prompts = PromptLoader()
        print(f"LLMProcessor ì´ˆê¸°í™” ì™„ë£Œ (model={self.model_name})")

        # 1. í•œêµ­ì–´ ìš”ì•½ì„ ê°•ì œí•˜ëŠ” PromptTemplate ì •ì˜
        KOREAN_SUMMARY_PROMPT = PromptTemplate(
            input_variables=["summary", "new_lines"],
            template="""ê¸°ì¡´ì˜ ëŒ€í™” ìš”ì•½ ì •ë³´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
            {summary}

            ìµœê·¼ì˜ ìƒˆë¡œìš´ ëŒ€í™” ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
            {new_lines}

            ìƒˆë¡œìš´ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìš”ì•½ ì •ë³´ë¥¼ **ë°˜ë“œì‹œ í•œêµ­ì–´**ë¡œ ì—…ë°ì´íŠ¸ í•´ì£¼ì„¸ìš”.
            ìµœì¢… ìš”ì•½ ì •ë³´:"""
        )

        # 2. ëŒ€í™” ë©”ëª¨ë¦¬ ì´ˆê¸°í™” (ìš”ì•½ ê¸°ë°˜)
        self.memory = ConversationSummaryMemory(
            llm=self.llm, 
            max_token_limit=1500,
            prompt=KOREAN_SUMMARY_PROMPT  # ì •ì˜í•œ í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ ì ìš©
        )
        self.chat_history = InMemoryChatMessageHistory()
        self.logger.info("ConversationSummaryMemory (í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ ì ìš©) ì´ˆê¸°í™” ì™„ë£Œ")
        # -------------------------------------------------------------

        # ConversationChain ìƒì„± (Memory + LLM)
        base_chain = ConversationChain(llm=self.llm, memory=self.memory, verbose=False)
        self.conversation_chain = RunnableWithMessageHistory(
            base_chain,
            lambda session_id: self.chat_history,
            input_messages_key="input",
            history_messages_key="history"
        )

    def generate_response(self, query: str, retrieved_chunks: Dict[str, Any]) -> str:
        """
        ê²€ìƒ‰ëœ ì²­í¬ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©í•˜ì—¬ LLM ì‘ë‹µ ìƒì„±

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            retrieved_chunks: search() ë˜ëŠ” search_page() ê²°ê³¼ (Dict[str, Any] ë˜ëŠ” List[Dict[str, Any]])
        
        Returns:
            str: LLM ì‘ë‹µ
        """
        self.logger.info(f"LLM ì‘ë‹µ ìƒì„± ì‹œì‘: query='{query[:50]}...'")

        # max_chunksëŠ” ì¼ë°˜ì ìœ¼ë¡œ configì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ íŒŒë¼ë¯¸í„°ë¡œ ë°›ì•„ì•¼ í•˜ì§€ë§Œ, 
        # í˜„ì¬ ì½”ë“œì—ì„œëŠ” ì •ì˜ë˜ì–´ ìˆì§€ ì•Šìœ¼ë¯€ë¡œ ì„ì‹œë¡œ 5ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜, 
        # generate_responseì˜ ì •ì˜ì—ì„œ max_chunks íŒŒë¼ë¯¸í„°ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
        # í˜„ì¬ í˜¸ì¶œ ì½”ë“œë¥¼ ë³´ë‹ˆ max_chunksê°€ ì „ë‹¬ë˜ì§€ ì•Šê³  ìˆìœ¼ë¯€ë¡œ,
        # self._build_context_from_... ë©”ì„œë“œì—ì„œ Noneì„ í—ˆìš©í•˜ë„ë¡ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        max_chunks = 5  # ê¸°ë³¸ê°’ ì„¤ì •

        # â­ï¸â­ï¸â­ï¸ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ë¡œì§ ìˆ˜ì • â­ï¸â­ï¸â­ï¸
        
        # 1. search_page() ê²°ê³¼ì¸ì§€ í™•ì¸: 'pages' í‚¤ê°€ ìˆê³  ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¼ ë•Œ
        if isinstance(retrieved_chunks, dict) and 'pages' in retrieved_chunks:
            # retrieved_chunks['pages']ë¥¼ ì‚¬ìš©í•˜ì—¬ _build_context_from_pages í˜¸ì¶œ
            pages = retrieved_chunks.get('pages', [])
            context = self._build_context_from_pages(pages, max_chunks)
        
        # 2. search() ê²°ê³¼ì¸ì§€ í™•ì¸: ì²­í¬ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¼ ë•Œ
        elif isinstance(retrieved_chunks, list):
            # retrieved_chunks ìì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ _build_context_from_chunks í˜¸ì¶œ
            context = self._build_context_from_chunks(retrieved_chunks, max_chunks)
        
        else:
            self.logger.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ìœ íš¨í•œ ë”•ì…”ë„ˆë¦¬(pages í¬í•¨) ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ í˜•íƒœê°€ ì•„ë‹™ë‹ˆë‹¤.")
            context = self.config.NO_CONTEXT_MESSAGE
        
        # â­ï¸â­ï¸â­ï¸ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ë¡œì§ ìˆ˜ì • ë â­ï¸â­ï¸â­ï¸


        if not context or context == self.config.NO_CONTEXT_MESSAGE:
            self.logger.warning("ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            context = self.config.NO_CONTEXT_MESSAGE
        
        self.logger.debug(f"context size = {len(context)}")

        # YAML ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë“œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        prompt_text = None

        # 1) PromptLoader.templates ë”•ì…”ë„ˆë¦¬ ìš°ì„  ê²€ì‚¬ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        if hasattr(self.prompts, "templates") and isinstance(self.prompts.templates, dict):
            for k in ("rag_prompt_template", "template", "default"):
                v = self.prompts.templates.get(k)
                if v:
                    prompt_text = v
                    break
            if not prompt_text:
                first = next(iter(self.prompts.templates.values()), None)
                if isinstance(first, dict):
                    prompt_text = first.get("rag_prompt_template") or first.get("template") or first.get("default")
                elif isinstance(first, str):
                    prompt_text = first

        # 2) load_template ë©”ì„œë“œê°€ ìˆìœ¼ë©´ íŒŒì¼ëª…ìœ¼ë¡œ ë¡œë“œ ì‹œë„ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        if not prompt_text and hasattr(self.prompts, "load_template"):
            try:
                tpl = self.prompts.load_template("prompt_temp_v1")
                if isinstance(tpl, dict):
                    prompt_text = tpl.get("rag_prompt_template") or tpl.get("template") or tpl.get("default")
            except Exception:
                pass

        # 3) ì„¤ì • íŒŒì¼(fallback) (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        prompt_text = prompt_text or getattr(self.config, "RAG_PROMPT_TEMPLATE", None)

        if not prompt_text:
            raise KeyError("'rag_prompt_template' ë˜ëŠ” ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # Note: ChatPromptTemplate.from_templateì„ ì‚¬ìš©í•˜ë©´ LangChain Chainì„ í†µí•˜ì§€ ì•Šê³  
        # ë°”ë¡œ LLMì„ í˜¸ì¶œí•  ë•Œ ë©”ëª¨ë¦¬ ì´ë ¥ì´ ë°˜ì˜ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
        # ì—¬ê¸°ì„œëŠ” ConversationChainì„ ì‚¬ìš©í•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ ë‘¡ë‹ˆë‹¤.
        
        # ConversationChainì„ í†µí•´ í˜¸ì¶œ (memory ë°˜ì˜)
        full_input = f"Context:\n{context}\n\nQuery:\n{query}"
        try:
            response = self.conversation_chain.invoke(
                {"input": full_input},
                config={"configurable": {"session_id": "default"}}
            )
            self.logger.info(f"LLM ì‘ë‹µ ìƒì„± ì™„ë£Œ")
            # LangChain RunnableWithMessageHistoryì˜ ê²°ê³¼ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
            return response["response"] if isinstance(response, dict) and "response" in response else str(response)
        except Exception as e:
            self.logger.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        
    def _build_context_from_chunks(
        self,
        chunks: List[Dict[str, Any]],
        max_chunks: Optional[int] = None
    ) -> str:
        """
        search() ê²°ê³¼ì—ì„œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            chunks: ì²­í¬ ë¦¬ìŠ¤íŠ¸
            max_chunks: ìµœëŒ€ í¬í•¨ ê°œìˆ˜
        """
        if max_chunks:
            chunks = chunks[:max_chunks]
        
        context_parts = []
        for i, chunk in enumerate(chunks, 1):
            file_name = chunk.get('file_name', 'unknown')
            chunk_text = chunk.get('text', '')
            
            context_parts.append(
                self.config.CONTEXT_FORMAT.format(
                    index=i,
                    file_name=file_name,
                    chunk_text=chunk_text
                )
            )
        
        self.logger.debug(f"ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (search): {len(chunks)}ê°œ ì²­í¬")
        return "\n\n".join(context_parts)


    def _build_context_from_pages(
        self,
        pages: List[Dict[str, Any]],
        max_chunks: Optional[int] = None
    ) -> str:
        """
        search_page() ê²°ê³¼ì—ì„œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            pages: í˜ì´ì§€ ë¦¬ìŠ¤íŠ¸
            max_chunks: ìµœëŒ€ í¬í•¨ ê°œìˆ˜
        """
        if max_chunks:
            pages = pages[:max_chunks]
        
        context_parts = []
        for i, page in enumerate(pages, 1):
            file_name = page.get('file_name', 'unknown')
            page_number = page.get('page_number', 0)
            page_text = page.get('text', '')
            score = page.get('score', 0.0)
            
            context_parts.append(
                f"[ë¬¸ì„œ {i}] {file_name} (í˜ì´ì§€ {page_number}, ìœ ì‚¬ë„: {score:.4f})\n{page_text}"
            )
        
        self.logger.debug(f"ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (search_page): {len(pages)}ê°œ í˜ì´ì§€")
        return "\n\n".join(context_parts)
        
    def get_memory_summary(self) -> str:
        """í˜„ì¬ê¹Œì§€ì˜ ëŒ€í™” ìš”ì•½ ë‚´ìš© ë°˜í™˜"""
        try:
            vars = self.memory.load_memory_variables({})
            history = vars.get("history", "")
            if not history.strip():
                return "(ì•„ì§ ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.)"
            return history
        except Exception as e:
            self.logger.error(f"ë©”ëª¨ë¦¬ ìš”ì•½ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return f"ë©”ëª¨ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {e}"